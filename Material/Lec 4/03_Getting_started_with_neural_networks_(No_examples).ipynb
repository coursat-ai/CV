{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03_Getting started with neural networks-(No examples).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AA87UzV72dS"
      },
      "source": [
        "# Chapter 3: Getting started with neural networks\n",
        "- NN Anatomy in depth\n",
        "- DL Frameworks:\n",
        "    - Why?\n",
        "    - Levels of frameworks\n",
        "- Setting up a DL machine (self-reading)\n",
        "- Introduction to Keras\n",
        "- Introduction to DL in cloud: Google Colab\n",
        "- Example 1: Classifying movie reviews (Binary classification)\n",
        "- Example 2: Classifying newswires (Multi-class classification)\n",
        "- Example 3: Predicting house prices (Regression)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uO298mrS72dT"
      },
      "source": [
        "## Anatomy of neural network in depth:\n",
        "Training a neural network revolves around the following\n",
        "objects:\n",
        "- Layers, which are combined into a network (or model)\n",
        "- The input data and corresponding targets\n",
        "- The loss function, which defines the feedback signal used for learning\n",
        "- The optimizer, which determines how learning proceeds\n",
        "\n",
        "![03_1_Anatomy_NN.png](https://github.com/ahmadelsallab/practical_dl/blob/master/Keras/notebooks/imgs/03_1_Anatomy_NN.png?raw=true)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjRt3M0872dV"
      },
      "source": [
        "### 1. Data\n",
        "\n",
        "### 2. Model:\n",
        "\n",
        "### Layers: the building blocks of deep learning\n",
        "A __layer__ is a data-processing module that takes as input one or more tensors and that outputs one or more tensors\n",
        "Some layers are stateless, but more frequently layers have a _state_: the layer’s __weights__, one or several tensors learned with stochastic gradient descent, which together contain the network’s _knowledge_.\n",
        "\n",
        "#### Dense layers --> Vector data (2D)\n",
        "simple vector data, stored in 2D tensors of shape `(samples, features)`, is often processed by densely connected layers, also called fully connected or dense layers (the _Dense_ class in Keras)\n",
        "\n",
        "#### Recurrent (LSTM) layers --> Sequence data (3D)\n",
        "Sequence data, stored in 3D tensors of shape `(samples, timesteps, features)`, is typically processed by recurrent layers such as an _LSTM_ layer.\n",
        "\n",
        "#### Convolution (Conv2D) layers --> Image data (4D)    \n",
        "Image data, stored in 4D tensors `(samples, channels, length, width)` or `(samples, length, width, channels)`, is usually processed by 2D convolution layers (_Conv2D_).\n",
        "\n",
        "\n",
        "### Layers and Data compatibility \n",
        "You can think of layers as the __LEGO__ bricks of deep learning, a metaphor that is made explicit by frameworks like Keras. Building deep-learning models in Keras is done by clipping together compatible layers to form useful data-transformation pipelines. The notion of layer compatibility here refers specifically to the fact that every layer will only accept input tensors of a certain shape and will return output tensors of a certain shape. Consider the following example:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoKAqvzg72dV"
      },
      "source": [
        "from keras import layers\n",
        "layer = layers.Dense(32, input_shape=(784,))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kI4i5Lht72da"
      },
      "source": [
        "We’re creating a layer that will only accept as input 2D tensors where the first dimension\n",
        "is 784 (axis 0, the batch dimension, is unspecified, and thus any value would be\n",
        "accepted). This layer will return a tensor where the first dimension has been transformed\n",
        "to be 32. Thus this layer can only be connected to a downstream layer that expects 32-\n",
        "dimensional vectors as its input. \n",
        "\n",
        "__When using Keras, you don’t have to worry about\n",
        "compatibility, because the layers you add to your models are dynamically built to\n",
        "match the shape of the incoming layer__. \n",
        "\n",
        "For instance, suppose you write the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyxVUXdM72db"
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(32, input_shape=(784,)))\n",
        "model.add(layers.Dense(32))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ty61bzwG72de"
      },
      "source": [
        "__The second layer didn’t receive an input shape argument__—instead, it automatically\n",
        "inferred its input shape as being the output shape of the layer that came before."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mid1LbrB72df"
      },
      "source": [
        "### Models: network of layers\n",
        "Model means a __network architecture__\n",
        "\n",
        "Using the above convention (which we will learn that is called sequential API), we can build stack of layers.\n",
        "But soon we will need to handle complex topologies:\n",
        "- Two-branch networks (Multi-inputs)\n",
        "- Multihead networks (Multi-outputs)\n",
        "- Multi-inputs, Multi-outputs\n",
        "- Inception blocks (Applying multiple parallel convolution layers to the same input).\n",
        "\n",
        "The topology of a network defines a _hypothesis space_ or _space of possibilities_ for mapping input data to output\n",
        "data.\n",
        "\n",
        "What you’ll then be searching for is a good set of values for the _weights_ tensors for that topology of layers.\n",
        "\n",
        "_Picking the right network architecture is more an art than a science_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdlOXYO272dg"
      },
      "source": [
        "## 3. Loss function (objective function)\n",
        "__The quantity that will be minimized during training. It represents a measure of success for the task at hand__\n",
        "\n",
        "### Choosing the right objective function for the right problem is extremely important\n",
        "\n",
        "your network will take any shortcut it can, to minimize the loss; so if the objective\n",
        "doesn’t fully correlate with success for the task at hand, your network will end up\n",
        "doing things you may not have wanted. Imagine a stupid, omnipotent AI trained via\n",
        "SGD, with this poorly chosen objective function: “maximizing the average well-being\n",
        "of all humans alive.” To make its job easier, this AI might choose to kill all humans\n",
        "except a few and focus on the well-being of the remaining ones—because average\n",
        "well-being isn’t affected by how many humans are left. That might not be what you\n",
        "intended! Just remember that all neural networks you build will be just as ruthless in\n",
        "lowering their loss function—so choose the objective wisely, or you’ll have to face\n",
        "unintended side effects.\n",
        "\n",
        "### Common loss functions\n",
        "- __binary crossentropy__ for a _two-class_ classification problem, \n",
        "- __categorical crossentropy__ for a _many-class_ classification problem,\n",
        "- __mean squared error__ for a _regression problem_\n",
        "- _connectionist temporal classification (__CTC__)_ for a _sequence-learning problem_\n",
        "\n",
        "_Only when you’re working on truly new research problems will you have to develop your own objective functions_\n",
        "\n",
        "\n",
        "### Multi-loss for multi-outputs\n",
        "A neural network that has multiple outputs may have multiple loss functions (one per\n",
        "output). But the gradient-descent process must be based on a single scalar loss value;\n",
        "so, for multiloss networks, all losses are combined (via averaging) into a single scalar\n",
        "quantity.\n",
        "\n",
        "## 4. Optimizer\n",
        "__Determines how the network will be updated based on the loss function. It implements a specific variant of stochastic gradient descent (SGD)__\n",
        "\n",
        "### Common optimizers:\n",
        "- __Adam__ the most commonly used.\n",
        "- __RMSProp__\n",
        "- __SGD__\n",
        "- ...etc\n",
        "\n",
        "For each one, you have __hyperparameters__ to adjust. The most important one is the __learning rate__. We will talk later about learning rate setting in more details.\n",
        "\n",
        " \n",
        "__Loss functions and optimizers: keys to configuring the learning process__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weI1P4Qs72dg"
      },
      "source": [
        "# Deep learning frameworks, why?\n",
        "## GPUs drive the revolution in DL\n",
        "![03_2_DL_GPU.png](https://github.com/ahmadelsallab/practical_dl/blob/master/Keras/notebooks/imgs/03_2_DL_GPU.png?raw=true)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sag4swrw72dh"
      },
      "source": [
        "## We need drivers for GPUs (CUDA)\n",
        "![03_3_NVIDIA_CUDA_Stack.png](https://github.com/ahmadelsallab/practical_dl/blob/master/Keras/notebooks/imgs/03_3_NVIDIA_CUDA_Stack.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYnyQbMk72dj"
      },
      "source": [
        "## Properties of DL frameworks:\n",
        "- Standard Interface, mostly Python\n",
        "- GPU and CPU compiler \n",
        "- Symbolic (auto) differentiation\n",
        "- BLAS optimization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXcuIIzq72dj"
      },
      "source": [
        "## Levels of frameworks:\n",
        "### Low level \n",
        "- Computations in general: Theano/Pytorch\n",
        "- Optimizers and NN names are not known\n",
        "- Framework calculates the gradients\n",
        "- You program does the optimization and weights update\n",
        "- Things like adaptive learning rates, momentum, early stopping..etc are done by you\n",
        "- Good if you want to come up with your optimization technique\n",
        "\n",
        "### Mid level \n",
        "- Optimization in general- TensorFlow/PyTorch\n",
        "- NN names are not known (ConvNets, RNN,…etc are unknown)\n",
        "- You choose the optimizer and objective\n",
        "- You write your own input-output relation _Model_\n",
        "- No control over internal programming of optimization technique\n",
        "- Good if you want to come up with a new model, NN connections,..etc\n",
        "\n",
        "### High level\n",
        "- Neural networks (Deep Learning): Keras, Caffee\n",
        "- NN layers are known--> lstm, convolution, dense,…etc\n",
        "- No control over the internal connections of layers\n",
        "- Good to have mix and match of known layers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MENYDQu72dl"
      },
      "source": [
        "# Introduction to Keras: applying the NN anatomy\n",
        "__Revision of earlier MNIST example__\n",
        "\n",
        "1. __Data__ Define your training data: input tensors and target tensors.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jmo2Hxcn72dm"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images = train_images.reshape((60000, 28 * 28))\n",
        "train_images = train_images.astype('float32') / 255\n",
        "test_images = test_images.reshape((10000, 28 * 28))\n",
        "test_images = test_images.astype('float32') / 255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emBh1ZfY72do"
      },
      "source": [
        "2. __Model__ Define a network of layers (or model ) that maps your inputs to your targets.\n",
        "\n",
        "There are two ways to define a model: \n",
        "\n",
        "__A. using the Sequential class__\n",
        "\n",
        "only for linear stacks of layers, which is the most common network architecture by far \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "As a refresher, here’s a two-layer model defined using the Sequential class (note\n",
        "that we’re passing the expected shape of the input data to the first layer):\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOEOGW0g72dq"
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(32, activation='relu', input_shape=(784,)))\n",
        "model.add(layers.Dense(10, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYdt3xkFM1Q-"
      },
      "source": [
        "__input_shape is mandatory!__\n",
        "\n",
        "try removing it. You get an error because we can't calc the output shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i554Hcc672dt"
      },
      "source": [
        "__B. using Functional API__ \n",
        "\n",
        "for directed acyclic graphs of layers, which lets you build completely arbitrary architectures\n",
        "\n",
        "- Two-branch networks (Multi-inputs)\n",
        "- Multihead networks (Multi-outputs)\n",
        "- Multi-inputs, Multi-outputs\n",
        "- Inception blocks (Applying multiple parallel convolution layers to the same input).\n",
        "\n",
        "And here’s the same model defined using the functional API:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfSF_e_072dv"
      },
      "source": [
        "input_tensor = layers.Input(shape=(784,))\n",
        "x = layers.Dense(32, activation='relu')(input_tensor)\n",
        "output_tensor = layers.Dense(10, activation='softmax')(x)\n",
        "model = models.Model(inputs=input_tensor, outputs=output_tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7NqC4Ash7Rm"
      },
      "source": [
        "# 2.2 Data representations for neural networks\n",
        "In the previous example, we started from data stored in multidimensional Numpy\n",
        "arrays, also called tensors. In general, all current machine-learning systems use tensors\n",
        "as their basic data structure.\n",
        "## What is a tensor?\n",
        "At its core, a tensor is a container for data—almost always numerical data. So, it’s a\n",
        "container for numbers. You may be already familiar with matrices, which are 2D tensors:\n",
        "tensors are a generalization of matrices to an arbitrary number of dimensions\n",
        "(note that in the context of tensors, a dimension is often called an axis).\n",
        "\n",
        "## Tensor dimension vs. rank\n",
        "Dimension = number of entries in an axis\n",
        "Rank = number of dimensions/axes\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJ0MXWXGh7Ro"
      },
      "source": [
        "## Key attributes of tensors:\n",
        "- Rank (.ndim)\n",
        "- Shape (.shape)\n",
        "- Data type (.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ej_u9gMYh7Rr"
      },
      "source": [
        "## Scalar (0D tensor)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxQXXJlCh7Rs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "64d98152-7545-4bf6-ecc5-e1abae288954"
      },
      "source": [
        "import numpy as np\n",
        "x = np.array(12)\n",
        "print(x.ndim)\n",
        "print(x.shape)\n",
        "print(x.dtype)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "()\n",
            "int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDMpP_wMh7R0"
      },
      "source": [
        "\n",
        "## Vector (1D tensor)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsLWZOxWh7R1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "b8b840de-7e3f-40c5-abc8-14f43bc8c5da"
      },
      "source": [
        "x = np.array([12, 3, 6, 14])\n",
        "print(x.ndim)\n",
        "print(x.shape)\n",
        "print(x.dtype)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "(4,)\n",
            "int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgLTP_C6h7R8"
      },
      "source": [
        "## Matrix (2D tensor)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlRkgcrjh7R-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "efc9ede0-894f-4f12-ae2c-d2f9f35c638a"
      },
      "source": [
        "x = np.array([[5, 78, 2, 34, 0], [6, 79, 3, 35, 1], [7, 80, 4, 36, 2]])\n",
        "print(x.ndim)\n",
        "print(x.shape)\n",
        "print(x.dtype)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "(3, 5)\n",
            "int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjOiJ8LYh7SD"
      },
      "source": [
        "### 3D tensors and higher-dimensional tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NCiQSamh7SE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "8f9ed293-3f6f-4421-ecb3-151b4c153b31"
      },
      "source": [
        "x = np.array([[[5, 78, 2, 34, 0],\n",
        "[6, 79, 3, 35, 1],\n",
        "[7, 80, 4, 36, 2]],\n",
        "[[5, 78, 2, 34, 0],\n",
        "[6, 79, 3, 35, 1],\n",
        "[7, 80, 4, 36, 2]],\n",
        "[[5, 78, 2, 34, 0],\n",
        "[6, 79, 3, 35, 1],\n",
        "[7, 80, 4, 36, 2]]])\n",
        "\n",
        "print(x)\n",
        "print(x.ndim)\n",
        "print(x.shape)\n",
        "print(x.dtype)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[ 5 78  2 34  0]\n",
            "  [ 6 79  3 35  1]\n",
            "  [ 7 80  4 36  2]]\n",
            "\n",
            " [[ 5 78  2 34  0]\n",
            "  [ 6 79  3 35  1]\n",
            "  [ 7 80  4 36  2]]\n",
            "\n",
            " [[ 5 78  2 34  0]\n",
            "  [ 6 79  3 35  1]\n",
            "  [ 7 80  4 36  2]]]\n",
            "3\n",
            "(3, 3, 5)\n",
            "int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9WR7Yb2h7SL"
      },
      "source": [
        "### Back to MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1j6yQWnh7SM"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJxa14JBh7SR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1fac9a6a-7a6e-47d4-e7ea-5d1ac371f97c"
      },
      "source": [
        "print(train_images.ndim)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXgzP6HHh7SZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1bd56bb4-887d-4c69-b525-72dd35913f3d"
      },
      "source": [
        "print(train_images.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ws4NdSAph7Sd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8232ed70-fd68-43ae-fee5-6b0679d61b16"
      },
      "source": [
        "print(train_images.dtype)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "uint8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yLg6Qtyh7Sl"
      },
      "source": [
        "So what we have here is a 3D tensor of 8-bit integers. More precisely, it’s an array of\n",
        "60,000 matrices of 28 × 8 integers. Each such matrix is a grayscale image, with coefficients\n",
        "between 0 and 255."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aubP76t-h7Sm"
      },
      "source": [
        "Let’s display the fourth digit in this 3D tensor, using the library Matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHEpEuOzh7So",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "e9c9447b-573a-400b-acf9-b22810adb69d"
      },
      "source": [
        "digit = train_images[4]\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "plt.imshow(digit, cmap=plt.cm.binary)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD4CAYAAADFJPs2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3cWi6u5h7Su"
      },
      "source": [
        "### Tensors slicing\n",
        "The following example selects digits #10 to #100 (#100 isn’t included) and puts\n",
        "them in an array of shape (90, 28, 28):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKtqWO1dh7Sv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "91e97720-fdec-40ff-fd86-48db8bb7ad33"
      },
      "source": [
        "my_slice = train_images[10:100]\n",
        "print(my_slice.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(90, 28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Fa67AR6h7Sz"
      },
      "source": [
        "It’s equivalent to this more detailed notation, which specifies a start index and stop\n",
        "index for the slice along each tensor axis. Note that : is equivalent to selecting the\n",
        "entire axis:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hy2YALtPh7S0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bd0b0c42-30d1-4bb6-c568-6648fc035a19"
      },
      "source": [
        "my_slice = train_images[10:100, :, :]\n",
        "print(my_slice.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(90, 28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a31sn4jWh7S4"
      },
      "source": [
        "In general, you may select between any two indices along each tensor axis. For\n",
        "instance, in order to select 14 × 14 pixels in the bottom-right corner of all images, you\n",
        "do this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceNg0Chph7S6"
      },
      "source": [
        "my_slice = train_images[:, 14:, 14:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdyQCdbIh7S9"
      },
      "source": [
        "It’s also possible to use negative indices. Much like negative indices in Python lists,\n",
        "they indicate a position relative to the end of the current axis. In order to crop the\n",
        "images to patches of 14 × 14 pixels centered in the middle, you do this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYaX9o_Zh7S-"
      },
      "source": [
        "my_slice = train_images[:, 7:-7, 7:-7]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJPLayvRh7TB"
      },
      "source": [
        "## Batches and tensor notations in Keras/TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mscNqL8dh7TD"
      },
      "source": [
        "### Dimension 0 = samples axis\n",
        "In general, the first axis (axis 0, because indexing starts at 0) in all data tensors you’ll\n",
        "come across in deep learning will be the samples axis (sometimes called the samples\n",
        "dimension). In the MNIST example, samples are images of digits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCW3ip1lOOwq"
      },
      "source": [
        "_Vector data (2D)_ simple vector data, stored in 2D tensors of shape `(samples, features)`.\n",
        "\n",
        "_Sequence data (3D)_ Sequence data, stored in 3D tensors of shape `(samples, timesteps, features)`\n",
        "\n",
        "_Image data (4D)_ Image data, stored in 4D tensors `(samples, channels, length, width)` or `(samples, length, width, channels)`, is usually processed by 2D convolution layers (_Conv2D_).\n",
        "\n",
        "_Video data (5D)_ tensors of shape (samples, frames, height, width, channels) or (samples, frames, channels, height, width)\n",
        "samples could be the batch (size=batch_size)\n",
        "\n",
        "### Length-Width convention in Math and Image\n",
        "Usually in Math and Linear algebra libraries like _numpy_ the 2D convention of a rectangle is `rows, cols`==> `width, length`.\n",
        "In image libraries, like _opencv_, _PIL_, the convention is `cols, rows`==>`width, length`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WS5M97jh7TD"
      },
      "source": [
        "### Batches \n",
        "In addition, deep-learning models don’t process an entire dataset at once; rather,\n",
        "they break the data into small batches. \n",
        "\n",
        "When considering such a batch tensor, the first axis (axis 0) is called the batch axis or\n",
        "batch dimension. This is a term you’ll frequently encounter when using Keras and other\n",
        "deep-learning libraries.\n",
        "\n",
        "Concretely, here’s one batch of our MNIST digits,\n",
        "with batch size of 128:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adq7rvrGh7TE"
      },
      "source": [
        "batch = train_images[:128]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOXDSOdMh7TI"
      },
      "source": [
        "And here’s the next batch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2h05436h7TJ"
      },
      "source": [
        "batch = train_images[128:256]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "re41Zjkmh7TN"
      },
      "source": [
        "And the nth batch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AehA0Ereh7TP"
      },
      "source": [
        "n = 10\n",
        "batch = train_images[128 * n:128 * (n + 1)]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21szrcN4h7TU"
      },
      "source": [
        "## Real-world examples of data tensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_vA0_Nih7TW"
      },
      "source": [
        "### Vector Data\n",
        "\n",
        "2D tensors of shape (samples, features)\n",
        "samples could be the batch (size=batch_size)\n",
        "\n",
        "\n",
        "Previously we called this Matrix (2D), which is still true.\n",
        "But remember, we talk here about each entry of data, where each row is a vector.\n",
        "We have array of Vector data, so it's a Matrix, each row is Vector data.\n",
        "\n",
        "This is the most common case. In such a dataset, each single data point can be encoded\n",
        "as a vector, and thus a batch of data will be encoded as a 2D tensor (that is, an array of\n",
        "vectors), where the first axis is the samples axis and the second axis is the features axis.\n",
        "Let’s take a look at two examples:\n",
        "\n",
        "-  A dataset of people, where we consider each person’s age, ZIP code,\n",
        "and income. Each person can be characterized as a vector of 3 values, and thus\n",
        "an entire dataset of 100,000 people can be stored in a 2D tensor of shape\n",
        "(100000, 3).\n",
        "-  A dataset of text documents, where we represent each document by the counts\n",
        "of how many times each word appears in it (out of a dictionary of 20,000 common\n",
        "words). Each document can be encoded as a vector of 20,000 values (one\n",
        "count per word in the dictionary), and thus an entire dataset of 500 documents\n",
        "can be stored in a tensor of shape (500, 20000)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Owby39LPh7TW"
      },
      "source": [
        "### Timeseries data or sequence data\n",
        "3D tensors of shape (samples, timesteps,\n",
        "features)\n",
        "samples could be the batch (size=batch_size)\n",
        "\n",
        "Whenever time matters in your data (or the notion of sequence order), it makes sense\n",
        "to store it in a 3D tensor with an explicit time axis. Each sample can be encoded as a\n",
        "sequence of vectors (a 2D tensor), and thus a batch of data will be encoded as a 3D\n",
        "tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnTsG7Jfh7TY"
      },
      "source": [
        "![02_3D tensors.png](https://github.com/ahmadelsallab/practical_dl/blob/master/Keras/notebooks/imgs/02_3D%20tensors.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60j51U0Lh7TZ"
      },
      "source": [
        "# The time axis is always the second axis (axis of index 1), by convention.\n",
        "\n",
        "Let’s look at a\n",
        "few examples:\n",
        "-  A dataset of stock prices. Every minute, we store the current price of the stock,\n",
        "the highest price in the past minute, and the lowest price in the past minute.\n",
        "Thus every minute is encoded as a 3D vector, an entire day of trading is\n",
        "encoded as a 2D tensor of shape (390, 3) (there are 390 minutes in a trading\n",
        "day), and 250 days’ worth of data can be stored in a 3D tensor of shape (250,\n",
        "390, 3). Here, each sample would be one day’s worth of data.\n",
        "- A dataset of tweets, where we encode each tweet as a sequence of 280 characters\n",
        "out of an alphabet of 128 unique characters. In this setting, each character can\n",
        "be encoded as a binary vector of size 128 (an all-zeros vector except for a 1 entry\n",
        "at the index corresponding to the character). Then each tweet can be encoded\n",
        "as a 2D tensor of shape (280, 128), and a dataset of 1 million tweets can be\n",
        "stored in a tensor of shape (1000000, 280, 128)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkOqA2g1h7Tb"
      },
      "source": [
        "### Image data\n",
        "4D tensors of shape (samples, height, width, channels) or (samples,\n",
        "channels, height, width)\n",
        "samples could be the batch (size=batch_size)\n",
        "\n",
        "Images typically have three dimensions: height, width, and color depth. Although\n",
        "grayscale images (like our MNIST digits) have only a single color channel and could\n",
        "thus be stored in 2D tensors, by convention image tensors are always 3D, with a onedimensional\n",
        "color channel for grayscale images. A batch of 128 grayscale images of\n",
        "size 256 × 256 could thus be stored in a tensor of shape (128, 256, 256, 1), and a\n",
        "batch of 128 color images could be stored in a tensor of shape (128, 256, 256, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--m1f4cRh7Te"
      },
      "source": [
        "![02_4D tensors.png](https://github.com/ahmadelsallab/practical_dl/blob/master/Keras/notebooks/imgs/02_4D%20tensors.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_SizOWnh7Th"
      },
      "source": [
        "There are two conventions for shapes of images tensors: the channels-last convention\n",
        "(used by TensorFlow) and the channels-first convention (used by Theano). The Tensor-\n",
        "Flow machine-learning framework, from Google, places the color-depth axis at the\n",
        "end: (samples, height, width, color_depth). Meanwhile, Theano places the color\n",
        "depth axis right after the batch axis: (samples, color_depth, height, width). With the Theano convention, the previous examples would become (128, 1, 256, 256)\n",
        "and (128, 3, 256, 256). The Keras framework provides support for both formats."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77ivf6cZh7Ti"
      },
      "source": [
        "### Video data\n",
        "5D tensors of shape (samples, frames, height, width, channels) or\n",
        "(samples, frames, channels, height, width)\n",
        "samples could be the batch (size=batch_size)\n",
        "\n",
        "Video data is one of the few types of real-world data for which you’ll need 5D tensors.\n",
        "A video can be understood as a sequence of frames, each frame being a color image.\n",
        "Because each frame can be stored in a 3D tensor (height, width, color_depth), a\n",
        "sequence of frames can be stored in a 4D tensor (frames, height, width, color_\n",
        "depth), and thus a batch of different videos can be stored in a 5D tensor of shape\n",
        "(samples, frames, height, width, color_depth)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeTiF6-3h7Tk"
      },
      "source": [
        "For instance, a 60-second, 144 × 256 YouTube video clip sampled at 4 frames per\n",
        "second would have 240 frames. A batch of four such video clips would be stored in a\n",
        "tensor of shape (4, 240, 144, 256, 3). That’s a total of 106,168,320 values! If the\n",
        "dtype of the tensor was float32, then each value would be stored in 32 bits, so the\n",
        "tensor would represent 405 MB. Heavy! Videos you encounter in real life are much\n",
        "lighter, because they aren’t stored in float32, and they’re typically compressed by a\n",
        "large factor (such as in the MPEG format)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_CQ5jV_qhb5"
      },
      "source": [
        "## Video CNN:\n",
        "\n",
        "https://colab.research.google.com/drive/1tdOYqhls2B35JxOc1TKKCJmxLvtv67s4#scrollTo=-UNwezHxKJ4O"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1D4ZF_yCh7Tl"
      },
      "source": [
        "## Tensors operations\n",
        "Neural networks perform transformations to input data.\n",
        "all transformations learned\n",
        "by deep neural networks can be reduced to a handful of tensor operations applied to\n",
        "tensors of numeric data. For instance, it’s possible to add tensors, multiply tensors,\n",
        "and so on.\n",
        "\n",
        "### A transformation is a Layer:\n",
        "In our initial example, we were building our network by stacking Dense layers on\n",
        "top of each other. A Keras layer instance looks like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0W0SujJCh7Tm"
      },
      "source": [
        "from keras.layers import Dense\n",
        "fully_connected_layer = Dense(512, activation='relu')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDHnfjnrh7Ts"
      },
      "source": [
        "This layer can be interpreted as a function, which takes as input a 2D tensor and\n",
        "returns another 2D tensor—a new representation for the input tensor. Specifically, the\n",
        "function is as follows (where W is a 2D tensor and b is a vector, both attributes of the\n",
        "layer):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmST1nED4ObS"
      },
      "source": [
        "```\n",
        "output = relu(dot(W, input) + b)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyM-rh7_h7Ty"
      },
      "source": [
        "Let’s unpack this. We have three tensor operations here: a dot product (dot) between\n",
        "the input tensor and a tensor named W; an addition (+) between the resulting 2D tensor\n",
        "and a vector b; and, finally, a relu operation. relu(x) is max(x, 0)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZq4sPA-h7T0"
      },
      "source": [
        "### Element-wise operations\n",
        "The relu operation and addition are element-wise operations: operations that are\n",
        "applied independently to each entry in the tensors being considered."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoCqt3tXh7T2"
      },
      "source": [
        "def naive_relu(x):\n",
        "    assert len(x.shape) == 2\n",
        "    x = x.copy()\n",
        "    for i in range(x.shape[0]):\n",
        "        for j in range(x.shape[1]):\n",
        "            x[i, j] = max(x[i, j], 0)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aE7_5GOZh7T6"
      },
      "source": [
        "def naive_add(x, y):\n",
        "    assert len(x.shape) == 2\n",
        "    assert x.shape == y.shape\n",
        "    x = x.copy()\n",
        "    for i in range(x.shape[0]):\n",
        "        for j in range(x.shape[1]):\n",
        "            x[i, j] += y[i, j]\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvFOcJu3h7T-"
      },
      "source": [
        "### But in reality it's actually parallel:\n",
        "This means\n",
        "these operations are highly amenable to massively parallel implementations (vectorized\n",
        "implementations, a term that comes from the vector processor supercomputer architecture\n",
        "from the 1970–1990 period)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_vGHYa0h7T_"
      },
      "source": [
        "## Try it yourself: measure the time for very long tensors (naive vs. GPU)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T16xqzX2h7UC"
      },
      "source": [
        "### BLAS\n",
        "In practice, when dealing with Numpy arrays, these operations are available as welloptimized\n",
        "built-in Numpy functions, which themselves delegate the heavy lifting to a\n",
        "Basic Linear Algebra Subprograms (BLAS) implementation if you have one installed\n",
        "(which you should). BLAS are low-level, highly parallel, efficient tensor-manipulation\n",
        "routines that are typically implemented in Fortran or C."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmmNCeJ8h7UD"
      },
      "source": [
        "# Tensor Dot\n",
        "Not same as element-wise\n",
        "Product is a scalar, for 2 vectors\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rg6_0uBFh7UE"
      },
      "source": [
        "def naive_vector_dot(x, y):\n",
        "    assert len(x.shape) == 1\n",
        "    assert len(y.shape) == 1\n",
        "    assert x.shape[0] == y.shape[0]\n",
        "    z = 0.\n",
        "    for i in range(x.shape[0]):\n",
        "        z += x[i] * y[i]\n",
        "    return z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oB82rmDHh7UO"
      },
      "source": [
        "For matrix and vector, dot produces a vector, with same number of elements = number of rows of the matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCpZruJuh7UP"
      },
      "source": [
        "For matrix and matrix, is a another matrix, with same number of rows as the first, and number of cols as the 2nd.\n",
        "#cols (1st) = #rows (2nd)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqGY-HcQh7UQ"
      },
      "source": [
        "![02_tensors dot.png](https://github.com/ahmadelsallab/practical_dl/blob/master/Keras/notebooks/imgs/02_tensors%20dot.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTxukTACh7UR"
      },
      "source": [
        "# Tensor reshaping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXWb8_7Rh7US"
      },
      "source": [
        "Reshaping a tensor means rearranging its rows and columns to match a target shape.\n",
        "Naturally, the reshaped tensor has the same total number of coefficients as the initial\n",
        "tensor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDZ_azFmh7UT"
      },
      "source": [
        "# Why?\n",
        "Image you have as the example an input like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdgVNZzph7UV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c0d7558b-5e60-4752-8d18-ef30116a392f"
      },
      "source": [
        "train_images.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9m3mXWzMh7UZ"
      },
      "source": [
        "Since the input layer is not conv, it cannot accept 2D input (Remember LEGO where inputs should match outputs shapes for layers compatibility)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBPFMgSZh7Ua"
      },
      "source": [
        "train_images = train_images.reshape((60000, 28 * 28))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea36oJCWh7Uf"
      },
      "source": [
        "# When to reshape and when to transpose?\n",
        "Reshape is usually used to re-order the elements from a shape to another. However, elements per axis is not necessarily preserved.\n",
        "\n",
        "Imagine that you have a vector data. As mentioned above, this type of data should be 2D: (samples, features).\n",
        "Suppose that your input is mis-ordered as (featrues, samples).\n",
        "Then you want to re-oreder the axes to match Keras convention.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJccKAvch7Uf"
      },
      "source": [
        "Using reshape:\n",
        "let n_samples = 3, n_features = 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8X8ZbOsh7Uh"
      },
      "source": [
        "import numpy as np\n",
        "sample_1 = [0., 1.]\n",
        "sample_2 = [2., 3.]\n",
        "sample_3 = [4., 5.]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEjFHwPFh7Uk"
      },
      "source": [
        "Say the samples are ordered column wise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiKE4-Okh7Ul",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1441a71d-693a-45ea-b8c1-486ee01f6418"
      },
      "source": [
        "x = np.array([[0., 2., 4.], [1., 3., 5.]])\n",
        "x.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cralNgiqh7Uv"
      },
      "source": [
        "Let's try to fix with reshape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "664nCTvch7Uw"
      },
      "source": [
        "x = x.reshape(3,2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zf_Lwjfh7Uz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1e53c47c-91f6-4ab7-d85a-4a8685605a26"
      },
      "source": [
        "x.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S24eyuwrh7U3"
      },
      "source": [
        "#### Is the original samples restored?\n",
        "No, why?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2QI6NAKh7U4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "4b5b14c4-1bff-43ef-a73e-bb11c9593831"
      },
      "source": [
        "x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 2.],\n",
              "       [4., 1.],\n",
              "       [3., 5.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nduwF-gWh7U9"
      },
      "source": [
        "#### Because reshape works col-wise, but doesnot preserve the axes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtjrRyB7h7U-"
      },
      "source": [
        "### Using transpose\n",
        "np.transpose will re-order the axes, keeping the element in each axis the same.\n",
        "So it's a special case of re-shape.\n",
        "We usually use it for axes re-ordering.\n",
        "\n",
        "it takes the new axes positions as input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TT817uYMh7U_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "8560eb50-6b52-4610-e628-ca385209cb99"
      },
      "source": [
        "x = np.array([[0., 2., 4.], [1., 3., 5.]])\n",
        "x = np.transpose(x)\n",
        "x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1.],\n",
              "       [2., 3.],\n",
              "       [4., 5.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWkb1z1Bh7VG"
      },
      "source": [
        "As you can see, the original samples are resotred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0bUNBvch7VH"
      },
      "source": [
        "#### Time series example\n",
        "Imagine that you have a time series data. As mentioned above, this type of data should be 3D: (samples, time, features).\n",
        "Suppose that your input is mis-ordered as (time, samples, features).\n",
        "Then you want to re-oreder the axes to match Keras convention.\n",
        "Here you use transpose as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHVxaxa8h7VJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "31a27cf9-dc85-4e55-e47c-a0807912d3cb"
      },
      "source": [
        "n_samples = 10\n",
        "n_timesteps = 5\n",
        "n_features = 3\n",
        "x = np.zeros([n_samples, n_features, n_timesteps])\n",
        "print(x.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10, 3, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEavvwYdh7VN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "526290db-b098-4dd1-9689-262362fca1f9"
      },
      "source": [
        "x = np.transpose(x, [0,2,1]) # also works x = x.transpose([0,2,1])\n",
        "x.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 5, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUSTi64eh7VU"
      },
      "source": [
        "### Notes:\n",
        "1. In 2D, np.transpose holds the same mathmatical meaning as matrix transpose, so no need to feed the axes order as they will normally be flipped\n",
        "2. Using reshape with the above 3D example will give the same shape, and most dangerously, the whole program will not give an error \n",
        "## But it will give wrong output and it's very hard to debug"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOXMT9Nuh7VW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "e56ef5c6-c3bc-4f8e-b48a-f1f74e72004f"
      },
      "source": [
        "x = np.zeros([n_samples, n_features, n_timesteps])\n",
        "print(x.shape)\n",
        "x = x.reshape([n_samples, n_timesteps, n_features])\n",
        "x.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10, 3, 5)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 5, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXMebHyMLSnV"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y16sYfXW72dy"
      },
      "source": [
        "## Numpy vs. Keras tensors:\n",
        "In function API, you treat layers as functions, which takes Keras tensor and returns another Keras tensor.\n",
        "Keras tensor is a __symbolic__ tensor. It means that it has no value _yet_. So if you try to print them you get a data structure specifying the name, shape, dimension, type,...\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnOv4aeu72dz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "488de2b6-7c22-4606-e6c5-a312c2dbf1da"
      },
      "source": [
        "print(output_tensor)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"dense_37/Softmax:0\", shape=(?, 10), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zcd44U4D72d2"
      },
      "source": [
        "\n",
        "This is different from Numpy tensors, which have immediate values. It also has shape, dimension, type."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEVi9Jsa72d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "294c71b4-2b52-4984-a948-ee24fbe68b78"
      },
      "source": [
        "import numpy as np\n",
        "x = np.array([[0., 2., 4.], [1., 3., 5.]])\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 2. 4.]\n",
            " [1. 3. 5.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ynrpxa072d5"
      },
      "source": [
        "## Static (Symboloic) computation graph\n",
        "__A model \n",
        "You might wonder, why we need two types of tensors?\n",
        "The answer is that, Keras is built of top of computation graph-based frameworks, like _TensorFlow_.\n",
        "In such frameworks, you first define an _abstract_ computation graph that defines the path from input to output.\n",
        "\n",
        "That graph is _static_.\n",
        "\n",
        "\n",
        "\n",
        "__Other frameworks like _Pytorch_ do not need those two types are they are based on _Dynamic_ graphs__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxJRR0IC72d6"
      },
      "source": [
        "__Once your model architecture is defined, it doesn’t matter whether you used a Sequential model or the functional API. All of the following steps are the same.__\n",
        "\n",
        "you can visualize your model using `model.summary`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJUnbFxc72d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "outputId": "a882842c-7f15-4db2-fc3d-ff8703c1718b"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_36 (Dense)             (None, 32)                25120     \n",
            "_________________________________________________________________\n",
            "dense_37 (Dense)             (None, 10)                330       \n",
            "=================================================================\n",
            "Total params: 25,450\n",
            "Trainable params: 25,450\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkDsOPXc72d-"
      },
      "source": [
        "3. __Compile__ Configure the learning process by choosing a loss function, an optimizer, and some metrics to monitor.\n",
        "\n",
        "The learning process is configured in the compilation step, where you specify the\n",
        "optimizer and loss function(s) that the model should use, as well as the metrics you\n",
        "want to monitor during training. \n",
        "\n",
        "Here’s an example with a single loss function, which\n",
        "is by far the most common case:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRBUYn4I72d-"
      },
      "source": [
        "from keras import optimizers\n",
        "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
        "                loss='mse',\n",
        "                metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBResmaL72eB"
      },
      "source": [
        "# Note!\n",
        "model.compile has nothing to do with weights, i.e. it doesnot re-init the weights. It only changes the optimizer, loss, metrics. \n",
        "It can be called several times after fit to change the lrate or even optimizer. \n",
        "If you want to re-init weights, then re-build all layers, then call compile on them. But better to call keras.backend.clear_session() before.\n",
        "Take care of the order of execution in ipython!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59QRyx1c72eC"
      },
      "source": [
        "4. __Fit__ Iterate on your training data by calling the fit() method of your model.\n",
        "\n",
        "__Your _Model_ is defined with symbolic tensors__\n",
        "\n",
        "__Your _Data_ is defined in Numpy tensors__\n",
        "\n",
        "Then you `fit()` your _Data_ to your _Model_ to train.\n",
        "You can think of this step as if you substitute the symbols in the symbolic graph with actual values.\n",
        "\n",
        "`fit()` initiates the training loop.\n",
        "\n",
        "Here you need to define the training parameters:\n",
        "- epochs\n",
        "- batch_size\n",
        "- validation_data\n",
        "- checkpoints (learning schedule, early stopping, best model saving, tensorboard,...etc)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "382lI0PfNeoB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "a3f7b8d1-7f84-4d81-e0d3-c460232fc6c1"
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "train_labels = to_categorical(train_labels)\n",
        "print(train_labels.shape)\n",
        "print(train_labels[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 10)\n",
            "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sSjCDZ672eF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "bca4a629-20c9-4c71-b62e-0f7c95a5694d"
      },
      "source": [
        "model.fit(train_images, train_labels, epochs=5, batch_size=128)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 0.0206 - acc: 0.8743\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 1s 22us/step - loss: 0.0109 - acc: 0.9310\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 1s 22us/step - loss: 0.0092 - acc: 0.9420\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 1s 22us/step - loss: 0.0081 - acc: 0.9485\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0074 - acc: 0.9537\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f03af3b4e48>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnbPhV6a72eH"
      },
      "source": [
        "# Running Deep Learning models:\n",
        "## Local mode: Setting up a deep-learning workstation\n",
        "Self reading: section 3.3\n",
        "    \n",
        "## Colab(Tutorial TODO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUPgytFn72eJ"
      },
      "source": [
        "# <center>More Real Examples</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYCLb2sI72eJ"
      },
      "source": [
        "# Example 1: Classifying movie reviews: a binary classification example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWrP2C_872eL"
      },
      "source": [
        "## 1. Data\n",
        "You’ll work with the IMDB dataset: a set of 50,000 highly polarized reviews from the\n",
        "Internet Movie Database. They’re split into 25,000 reviews for training and 25,000\n",
        "reviews for testing, each set consisting of 50% negative and 50% positive reviews.\n",
        "\n",
        "__Why use separate training and test sets?__\n",
        "\n",
        "Because you should never test a machinelearning\n",
        "model on the same data that you used to train it! Just because a model performs\n",
        "well on its training data doesn’t mean it will perform well on data it has never\n",
        "seen; and what you care about is your model’s performance on new data (because you\n",
        "already know the labels of your training data—obviously you don’t need your model\n",
        "to predict those). For instance, it’s possible that your model could end up merely memorizing\n",
        "a mapping between your training samples and their targets, which would be\n",
        "useless for the task of predicting targets for data the model has never seen before.\n",
        "We’ll go over this point in much more detail in the next chapter.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RA7mUeL72eN"
      },
      "source": [
        "## 1.1 Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYh71hUf72eP"
      },
      "source": [
        "from keras.datasets import imdb\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)# The argument num_words=10000 means you’ll only keep the top 10,000 most frequentlyoccurring words in the training data. Rare words will be discarded. This allows you to work with vector data of manageable size."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CqQ9cuJ72eT"
      },
      "source": [
        "The variables `train_data` and `test_data` are lists of reviews; each review is a list of\n",
        "word indices (encoding a sequence of words). train_labels and test_labels are lists of 0s and 1s, where 0 stands for negative and 1 stands for positive:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXfQA-Hs72eU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3981
        },
        "outputId": "41b6f00a-9c30-4f2d-aad3-3ec73e1a90e4"
      },
      "source": [
        "train_data[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 14,\n",
              " 22,\n",
              " 16,\n",
              " 43,\n",
              " 530,\n",
              " 973,\n",
              " 1622,\n",
              " 1385,\n",
              " 65,\n",
              " 458,\n",
              " 4468,\n",
              " 66,\n",
              " 3941,\n",
              " 4,\n",
              " 173,\n",
              " 36,\n",
              " 256,\n",
              " 5,\n",
              " 25,\n",
              " 100,\n",
              " 43,\n",
              " 838,\n",
              " 112,\n",
              " 50,\n",
              " 670,\n",
              " 2,\n",
              " 9,\n",
              " 35,\n",
              " 480,\n",
              " 284,\n",
              " 5,\n",
              " 150,\n",
              " 4,\n",
              " 172,\n",
              " 112,\n",
              " 167,\n",
              " 2,\n",
              " 336,\n",
              " 385,\n",
              " 39,\n",
              " 4,\n",
              " 172,\n",
              " 4536,\n",
              " 1111,\n",
              " 17,\n",
              " 546,\n",
              " 38,\n",
              " 13,\n",
              " 447,\n",
              " 4,\n",
              " 192,\n",
              " 50,\n",
              " 16,\n",
              " 6,\n",
              " 147,\n",
              " 2025,\n",
              " 19,\n",
              " 14,\n",
              " 22,\n",
              " 4,\n",
              " 1920,\n",
              " 4613,\n",
              " 469,\n",
              " 4,\n",
              " 22,\n",
              " 71,\n",
              " 87,\n",
              " 12,\n",
              " 16,\n",
              " 43,\n",
              " 530,\n",
              " 38,\n",
              " 76,\n",
              " 15,\n",
              " 13,\n",
              " 1247,\n",
              " 4,\n",
              " 22,\n",
              " 17,\n",
              " 515,\n",
              " 17,\n",
              " 12,\n",
              " 16,\n",
              " 626,\n",
              " 18,\n",
              " 2,\n",
              " 5,\n",
              " 62,\n",
              " 386,\n",
              " 12,\n",
              " 8,\n",
              " 316,\n",
              " 8,\n",
              " 106,\n",
              " 5,\n",
              " 4,\n",
              " 2223,\n",
              " 5244,\n",
              " 16,\n",
              " 480,\n",
              " 66,\n",
              " 3785,\n",
              " 33,\n",
              " 4,\n",
              " 130,\n",
              " 12,\n",
              " 16,\n",
              " 38,\n",
              " 619,\n",
              " 5,\n",
              " 25,\n",
              " 124,\n",
              " 51,\n",
              " 36,\n",
              " 135,\n",
              " 48,\n",
              " 25,\n",
              " 1415,\n",
              " 33,\n",
              " 6,\n",
              " 22,\n",
              " 12,\n",
              " 215,\n",
              " 28,\n",
              " 77,\n",
              " 52,\n",
              " 5,\n",
              " 14,\n",
              " 407,\n",
              " 16,\n",
              " 82,\n",
              " 2,\n",
              " 8,\n",
              " 4,\n",
              " 107,\n",
              " 117,\n",
              " 5952,\n",
              " 15,\n",
              " 256,\n",
              " 4,\n",
              " 2,\n",
              " 7,\n",
              " 3766,\n",
              " 5,\n",
              " 723,\n",
              " 36,\n",
              " 71,\n",
              " 43,\n",
              " 530,\n",
              " 476,\n",
              " 26,\n",
              " 400,\n",
              " 317,\n",
              " 46,\n",
              " 7,\n",
              " 4,\n",
              " 2,\n",
              " 1029,\n",
              " 13,\n",
              " 104,\n",
              " 88,\n",
              " 4,\n",
              " 381,\n",
              " 15,\n",
              " 297,\n",
              " 98,\n",
              " 32,\n",
              " 2071,\n",
              " 56,\n",
              " 26,\n",
              " 141,\n",
              " 6,\n",
              " 194,\n",
              " 7486,\n",
              " 18,\n",
              " 4,\n",
              " 226,\n",
              " 22,\n",
              " 21,\n",
              " 134,\n",
              " 476,\n",
              " 26,\n",
              " 480,\n",
              " 5,\n",
              " 144,\n",
              " 30,\n",
              " 5535,\n",
              " 18,\n",
              " 51,\n",
              " 36,\n",
              " 28,\n",
              " 224,\n",
              " 92,\n",
              " 25,\n",
              " 104,\n",
              " 4,\n",
              " 226,\n",
              " 65,\n",
              " 16,\n",
              " 38,\n",
              " 1334,\n",
              " 88,\n",
              " 12,\n",
              " 16,\n",
              " 283,\n",
              " 5,\n",
              " 16,\n",
              " 4472,\n",
              " 113,\n",
              " 103,\n",
              " 32,\n",
              " 15,\n",
              " 16,\n",
              " 5345,\n",
              " 19,\n",
              " 178,\n",
              " 32]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNWd61OX72eW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "90e06b73-ab83-4d61-9a79-fa7d7e7ca099"
      },
      "source": [
        "train_labels[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3NNCi8M72eZ"
      },
      "source": [
        "Because you’re restricting yourself to the top 10,000 most frequent words, no word\n",
        "index will exceed 10,000:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqW5xKVq72ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ef47445b-5ba3-4d64-a59b-b7976785e483"
      },
      "source": [
        "max([max(sequence) for sequence in train_data])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9999"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ga7w-3tH72ed"
      },
      "source": [
        "### Sanity checks\n",
        "For kicks, here’s how you can quickly decode one of these reviews back to English\n",
        "words:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fw4Rmd_j72ee"
      },
      "source": [
        "word_index = imdb.get_word_index()\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMxMQqoG72ej"
      },
      "source": [
        "## 1.2 Prepare data (vectorize, binarize, digitize,...etc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ca6Gzoe372el"
      },
      "source": [
        "You can’t feed lists of integers into a neural network. You have to turn your lists into\n",
        "tensors. There are two ways to do that:\n",
        "\n",
        "-  Pad your lists so that they all have the same length, turn them into an integer\n",
        "tensor of shape (samples, word_indices), and then use as the first layer in\n",
        "your network a layer capable of handling such integer tensors (the Embedding\n",
        "layer, which we’ll cover in detail later in the book).\n",
        "\n",
        "- One-hot encode your lists to turn them into vectors of 0s and 1s. This would\n",
        "mean, for instance, turning the sequence [3, 5] into a 10,000-dimensional vector\n",
        "that would be all 0s except for indices 3 and 5, which would be 1s. Then you\n",
        "could use as the first layer in your network a Dense layer, capable of handling\n",
        "floating-point vector data.\n",
        "\n",
        "_We will use the latter one for simplicity, but you should have noticed that the order of words is mixed. This model is called Bag-of-words_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MO8ePxrl72em"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        results[i, sequence] = 1.\n",
        "    return results\n",
        "\n",
        "x_train = vectorize_sequences(train_data)\n",
        "x_test = vectorize_sequences(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2D5b2Fho72ep"
      },
      "source": [
        "Let's have a look on the data transformation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BORfDzxk-3Ck",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2c95f571-32d7-4dd4-b2ea-f68dab6c4e28"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 10000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JwKToS972ep",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "52fce9d7-9fc7-4902-8245-73526417c970"
      },
      "source": [
        "x_train[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 1., 1., ..., 0., 0., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgrQFa_c72et"
      },
      "source": [
        "### Labels binarization\n",
        "You should also vectorize your labels, which is straightforward for _binary classification_:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceEebH6q72eu"
      },
      "source": [
        "y_train = np.asarray(train_labels).astype('float32')\n",
        "y_test = np.asarray(test_labels).astype('float32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJItVJ3x72ew"
      },
      "source": [
        "Now the data is ready to be fed into a neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfQwdawm72ey"
      },
      "source": [
        "## 2. Model\n",
        "Since the input is a 2D vector, then we choose `Dense` layers. Remeber, our choice of the BoW model.\n",
        "For simplicity, we will be using the Sequential API.\n",
        "\n",
        "Let's choose the `relu` for activation function, which is the most widely used choice (Activation functions to be revisited later).\n",
        "\n",
        "The argument being passed to each Dense layer (16) is the number of hidden units of the layer. A _hidden unit_ is a dimension in the representation space of the layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmtrVHLY72e0"
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Omiec_8t72e3"
      },
      "source": [
        "You may remember from chapter 2 that each such Dense layer with a relu activation\n",
        "implements the following chain of tensor operations `output = relu(dot(W, input) + b)`\n",
        "\n",
        "__Network Capacity = degrees of freedom__\n",
        "Having 16 hidden units means the weight matrix W will have shape (input_dimension,\n",
        "16): the dot product with W will project the input data onto a 16-dimensional representation\n",
        "space (and then you’ll add the bias vector b and apply the relu operation). You\n",
        "can intuitively understand the dimensionality of your representation space as “how\n",
        "much freedom you’re allowing the network to have when learning internal representations.”\n",
        "\n",
        "__Capacity vs. Complexity__\n",
        "Having more hidden units (a higher-dimensional representation space)\n",
        "allows your network to learn more-complex representations, but it makes the network\n",
        "more computationally expensive and may lead to learning unwanted patterns (patterns\n",
        "that will improve performance on the training data but not on the test data).\n",
        "\n",
        "__Network architecture definition__\n",
        "There are two key architecture decisions to be made about such a stack of Dense layers:\n",
        "- How many layers to use\n",
        "- How many hidden units to choose for each layer\n",
        "\n",
        "Now, to have a model, we need to have _Network of layers_. For the Sequential API, we can only do stack of layers.\n",
        "In next lessons, you’ll learn formal principles to guide you in making these choices. For\n",
        "the time being, you’ll have to trust me with the following architecture choice:\n",
        "\n",
        "- Two intermediate layers with 16 hidden units each\n",
        "- A third layer that will output the scalar prediction regarding the sentiment of the current review\n",
        "\n",
        "![03_4_Simple_Model.png](https://github.com/ahmadelsallab/practical_dl/blob/master/Keras/notebooks/imgs/03_4_Simple_Model.png?raw=true)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olox9qgy72e4"
      },
      "source": [
        "model.add(layers.Dense(16, activation='relu'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEWG-cjU72e7"
      },
      "source": [
        "![03_5_relul.png](https://github.com/ahmadelsallab/practical_dl/blob/master/Keras/notebooks/imgs/03_5_relu.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GomHWx772e8"
      },
      "source": [
        "The final layer will use a sigmoid activation so as to output a probability (a score between 0 and 1),indicating how likely the sample is to have the target “1”: how likely the review is to be\n",
        "positive)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rs0TdF9p72e9"
      },
      "source": [
        "model.add(layers.Dense(1, activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2D_qwT15ZvV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnOpMHU-72e-"
      },
      "source": [
        "![03_6_sigmoid.png](https://github.com/ahmadelsallab/practical_dl/blob/master/Keras/notebooks/imgs/03_6_sigmoid.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTmMmRiw72e_"
      },
      "source": [
        "__What are activation functions, and why are they necessary?__\n",
        "\n",
        "Without an activation function like relu (also called a non-linearity), the Dense layer\n",
        "would consist of two linear operations—a dot product and an addition:\n",
        "\n",
        "`output = dot(W, input) + b`\n",
        "\n",
        "So the layer could only learn _linear transformations_ (affine transformations) of the\n",
        "input data: the hypothesis space of the layer would be the set of all possible linear\n",
        "transformations of the input data into a 16-dimensional space. \n",
        "\n",
        "Such a hypothesis space is too restricted and wouldn’t benefit from multiple layers of representations,\n",
        "because a deep stack of linear layers would still implement a linear operation: \n",
        "__adding more layers wouldn’t extend the hypothesis space.__\n",
        "\n",
        "In order to get access to a much richer hypothesis space that would benefit from\n",
        "deep representations, you need a non-linearity, or activation function. _relu is the\n",
        "most popular activation function in deep learning_, but there are many other candidates,\n",
        "which all come with similarly strange names: _prelu, elu_, and so on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGx6Wrm772fA"
      },
      "source": [
        "# 3. Compile\n",
        "## loss \n",
        "Because you’re facing a\n",
        "binary classification problem and the output of your network is a probability (you end\n",
        "your network with a single-unit layer with a sigmoid activation), it’s best to use the _binary_crossentropy_ loss. It isn’t the only viable choice: you could use, for instance,\n",
        "_mean_squared_error_. \n",
        "\n",
        "_But crossentropy is usually the best choice when you’re dealing\n",
        "with models that output probabilities._\n",
        "\n",
        "Crossentropy is a quantity from the field of _Information Theory_ that measures the distance between probability distributions or, in this case, between the ground-truth distribution and your predictions.\n",
        "\n",
        "## Optimizer\n",
        "Let's use RMSProp\n",
        "\n",
        "## Complie\n",
        "Here’s the step where you configure the model with the rmsprop optimizer and\n",
        "the binary_crossentropy loss function. Note that you’ll also monitor accuracy\n",
        "during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZpHWMfn72fA"
      },
      "source": [
        "model.compile(optimizer='rmsprop',\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7s7-x6672fC"
      },
      "source": [
        "__Another way of passing args to compile__\n",
        "\n",
        "You’re passing your optimizer, loss function, and metrics as strings, which is possible\n",
        "because rmsprop, binary_crossentropy, and accuracy are packaged as part of Keras.\n",
        "Sometimes you may want to configure the parameters of your optimizer or pass a custom\n",
        "loss function or metric function. The former can be done by passing an optimizer\n",
        "class instance as the optimizer argument,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwdqQc1D72fC"
      },
      "source": [
        "from keras import optimizers\n",
        "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
        "loss='binary_crossentropy',\n",
        "metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmnE0TX472fF"
      },
      "source": [
        "from keras import losses\n",
        "from keras import metrics\n",
        "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
        "loss=losses.binary_crossentropy,\n",
        "metrics=[metrics.binary_accuracy])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9-O6Qx872fH"
      },
      "source": [
        "## Validation\n",
        "In order to monitor during training the accuracy of the model on data it has never\n",
        "seen before, you’ll create a validation set by setting apart 10,000 samples from the\n",
        "original training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7f1J37g72fR"
      },
      "source": [
        "x_val = x_train[:10000]\n",
        "partial_x_train = x_train[10000:]\n",
        "y_val = y_train[:10000]\n",
        "partial_y_train = y_train[10000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXZ6IbUM72fT"
      },
      "source": [
        "# 4. Fit\n",
        "\n",
        "You’ll now train the model for 20 epochs (20 iterations over all samples in the\n",
        "x_train and y_train tensors), in mini-batches of 512 samples. At the same time,\n",
        "you’ll monitor loss and accuracy on the 10,000 samples that you set apart. You do so by\n",
        "passing the validation data as the validation_data argument"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXNxM7rD72fU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 783
        },
        "outputId": "d4c4b38b-e472-4379-f3ed-6aab45719887"
      },
      "source": [
        "history = model.fit(partial_x_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=20,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(x_val, y_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 15000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "15000/15000 [==============================] - 3s 215us/step - loss: 0.5048 - binary_accuracy: 0.7871 - val_loss: 0.3774 - val_binary_accuracy: 0.8705\n",
            "Epoch 2/20\n",
            "15000/15000 [==============================] - 2s 156us/step - loss: 0.2991 - binary_accuracy: 0.9048 - val_loss: 0.3001 - val_binary_accuracy: 0.8897\n",
            "Epoch 3/20\n",
            "15000/15000 [==============================] - 2s 158us/step - loss: 0.2173 - binary_accuracy: 0.9283 - val_loss: 0.3082 - val_binary_accuracy: 0.8717\n",
            "Epoch 4/20\n",
            "15000/15000 [==============================] - 2s 157us/step - loss: 0.1747 - binary_accuracy: 0.9437 - val_loss: 0.2827 - val_binary_accuracy: 0.8844\n",
            "Epoch 5/20\n",
            "15000/15000 [==============================] - 2s 159us/step - loss: 0.1421 - binary_accuracy: 0.9539 - val_loss: 0.2856 - val_binary_accuracy: 0.8855\n",
            "Epoch 6/20\n",
            "15000/15000 [==============================] - 2s 158us/step - loss: 0.1148 - binary_accuracy: 0.9651 - val_loss: 0.3141 - val_binary_accuracy: 0.8777\n",
            "Epoch 7/20\n",
            "15000/15000 [==============================] - 2s 158us/step - loss: 0.0977 - binary_accuracy: 0.9708 - val_loss: 0.3131 - val_binary_accuracy: 0.8841\n",
            "Epoch 8/20\n",
            "15000/15000 [==============================] - 2s 158us/step - loss: 0.0805 - binary_accuracy: 0.9764 - val_loss: 0.3864 - val_binary_accuracy: 0.8653\n",
            "Epoch 9/20\n",
            "15000/15000 [==============================] - 2s 157us/step - loss: 0.0660 - binary_accuracy: 0.9820 - val_loss: 0.3642 - val_binary_accuracy: 0.8774\n",
            "Epoch 10/20\n",
            "15000/15000 [==============================] - 2s 157us/step - loss: 0.0559 - binary_accuracy: 0.9847 - val_loss: 0.3864 - val_binary_accuracy: 0.8778\n",
            "Epoch 11/20\n",
            "15000/15000 [==============================] - 2s 158us/step - loss: 0.0434 - binary_accuracy: 0.9901 - val_loss: 0.4181 - val_binary_accuracy: 0.8768\n",
            "Epoch 12/20\n",
            "15000/15000 [==============================] - 2s 158us/step - loss: 0.0379 - binary_accuracy: 0.9913 - val_loss: 0.4556 - val_binary_accuracy: 0.8698\n",
            "Epoch 13/20\n",
            "15000/15000 [==============================] - 2s 159us/step - loss: 0.0290 - binary_accuracy: 0.9941 - val_loss: 0.4719 - val_binary_accuracy: 0.8733\n",
            "Epoch 14/20\n",
            "15000/15000 [==============================] - 2s 158us/step - loss: 0.0257 - binary_accuracy: 0.9942 - val_loss: 0.5070 - val_binary_accuracy: 0.8714\n",
            "Epoch 15/20\n",
            "15000/15000 [==============================] - 2s 158us/step - loss: 0.0191 - binary_accuracy: 0.9968 - val_loss: 0.5340 - val_binary_accuracy: 0.8706\n",
            "Epoch 16/20\n",
            "15000/15000 [==============================] - 2s 162us/step - loss: 0.0146 - binary_accuracy: 0.9983 - val_loss: 0.5727 - val_binary_accuracy: 0.8691\n",
            "Epoch 17/20\n",
            "15000/15000 [==============================] - 2s 160us/step - loss: 0.0146 - binary_accuracy: 0.9977 - val_loss: 0.6020 - val_binary_accuracy: 0.8675\n",
            "Epoch 18/20\n",
            "15000/15000 [==============================] - 2s 158us/step - loss: 0.0090 - binary_accuracy: 0.9992 - val_loss: 0.6571 - val_binary_accuracy: 0.8642\n",
            "Epoch 19/20\n",
            "15000/15000 [==============================] - 2s 157us/step - loss: 0.0059 - binary_accuracy: 0.9998 - val_loss: 0.7417 - val_binary_accuracy: 0.8543\n",
            "Epoch 20/20\n",
            "15000/15000 [==============================] - 2s 155us/step - loss: 0.0057 - binary_accuracy: 0.9997 - val_loss: 0.7396 - val_binary_accuracy: 0.8592\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckMXv84R72fX"
      },
      "source": [
        "On CPU, this will take less than 2 seconds per epoch—training is over in 20 seconds.\n",
        "At the end of every epoch, there is a slight pause as the model computes its loss and\n",
        "accuracy on the 10,000 samples of the validation data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itZC9Xe372fY"
      },
      "source": [
        "## Plotting the loss\n",
        "__History object__\n",
        "\n",
        "Note that the call to model.fit() returns a History object. This object has a member\n",
        "history, which is a dictionary containing data about everything that happened\n",
        "during training. Let’s look at it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOKS9ySA72fZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "267242f1-d804-4c60-b1dd-c120478f8944"
      },
      "source": [
        "history_dict = history.history\n",
        "history_dict.keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['val_loss', 'val_binary_accuracy', 'loss', 'binary_accuracy'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4Z9aQXM72fe"
      },
      "source": [
        "The dictionary contains four entries: one per metric that was being monitored during\n",
        "training and during validation. In the following two listing, let’s use Matplotlib to plot\n",
        "the training and validation loss side by side"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6_zsYUo72fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "outputId": "4d18b5e7-f99f-49ae-a75b-f2990ab0f874"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "history_dict = history.history\n",
        "loss_values = history_dict['loss']\n",
        "val_loss_values = history_dict['val_loss']\n",
        "epochs = range(1, len(val_loss_values) + 1)\n",
        "plt.plot(epochs, loss_values, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFnCAYAAACPasF4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFEaCtxBCCKEw/wCluHY0xlz7PgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLyOJH6s72fh"
      },
      "source": [
        "## We can do the same with accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3pihUCS72fi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "outputId": "aed7f120-0bb3-466b-e16d-30cacde73310"
      },
      "source": [
        "plt.clf()\n",
        "acc_values = history_dict['binary_accuracy']\n",
        "val_acc_values = history_dict['val_binary_accuracy']\n",
        "plt.plot(epochs, acc_values, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc_values, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAFnCAYAAAC/5tBZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlDSJAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ot-x-eT72fl"
      },
      "source": [
        "## We are overfitting!\n",
        "As you can see, the training loss decreases with every epoch, and the training accuracy\n",
        "increases with every epoch. That’s what you would expect when running gradientdescent\n",
        "optimization—the quantity you’re trying to minimize should be less with\n",
        "every iteration. But that isn’t the case for the validation loss and accuracy: they seem to\n",
        "peak at the fourth epoch. This is an example of what we warned against earlier: a\n",
        "model that performs better on the training data isn’t necessarily a model that will do\n",
        "better on data it has never seen before. In precise terms, what you’re seeing is overfitting:\n",
        "after the second epoch, you’re overoptimizing on the training data, and you end\n",
        "up learning representations that are specific to the training data and don’t generalize\n",
        "to data outside of the training set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSIJIro_72fm"
      },
      "source": [
        "__How to stop it__\n",
        "\n",
        "In this case, to prevent overfitting, you could stop training after three epochs. In\n",
        "\n",
        "general, you can use a range of techniques to mitigate overfitting, which we’ll cover later.\n",
        "\n",
        "Let's re-train again:\n",
        "\n",
        "__Important note__\n",
        "In ipynb, you can't just call fit for epochs=3, because this will train over the last model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MT1OKNL072fn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "outputId": "4241a077-3104-4923-8eb7-eb3ffc5f5a8d"
      },
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(16, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='rmsprop',\n",
        "loss='binary_crossentropy',\n",
        "metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "model.fit(x_train, y_train, epochs=4, batch_size=512)\n",
        "results = model.evaluate(x_test, y_test)\n",
        "results"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_41 (Dense)             (None, 16)                160016    \n",
            "_________________________________________________________________\n",
            "dense_42 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dense_43 (Dense)             (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 160,305\n",
            "Trainable params: 160,305\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/4\n",
            "25000/25000 [==============================] - 4s 140us/step - loss: 0.4737 - acc: 0.8219\n",
            "Epoch 2/4\n",
            "25000/25000 [==============================] - 3s 111us/step - loss: 0.2674 - acc: 0.9092\n",
            "Epoch 3/4\n",
            "25000/25000 [==============================] - 3s 112us/step - loss: 0.2034 - acc: 0.9281\n",
            "Epoch 4/4\n",
            "25000/25000 [==============================] - 3s 113us/step - loss: 0.1716 - acc: 0.9385\n",
            "25000/25000 [==============================] - 2s 97us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3110877853107452, 0.87612]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33g-EeAk72fp"
      },
      "source": [
        "This fairly naive approach achieves an accuracy of 88%. With state-of-the-art\n",
        "approaches, you should be able to get close to 95%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3v0gZbJ72fr"
      },
      "source": [
        "# Inference\n",
        "__Using a trained network to generate predictions on new data__\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfED9pk272fr"
      },
      "source": [
        "out = model.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "in9sHUogDz8h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "a836da23-c9cc-47a9-8f15-ee0cd9a03bde"
      },
      "source": [
        "out[10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.82113326], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Anx4Etei72fu"
      },
      "source": [
        "# Further things to try:\n",
        "The following experiments will help convince you that the architecture choices you’ve\n",
        "made are all fairly reasonable, although there’s still room for improvement:\n",
        "- You used two hidden layers. Try using one or three hidden layers, and see how\n",
        "doing so affects validation and test accuracy.\n",
        "- Try using layers with more hidden units or fewer hidden units: 32 units, 64 units,\n",
        "and so on.\n",
        "- Try using the mse loss function instead of binary_crossentropy.\n",
        "- Try using the tanh activation (an activation that was popular in the early days of\n",
        "neural networks) instead of relu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjUjRb5Q72fv"
      },
      "source": [
        "# Example 2: Classifying newswires (Multi-class classification)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfwywOfM72fw"
      },
      "source": [
        "# Example 3: Predicting house prices (Regression)"
      ]
    }
  ]
}