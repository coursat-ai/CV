{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04_Fundamentals of machine learning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNeYyuhZS0os"
      },
      "source": [
        "# Chapter 4: Fundamentals of machine learning\n",
        "- Banches of ML\n",
        "- Glossary of ML\n",
        "- Evaluation protocols\n",
        "- Data pre-processing \n",
        "- Features Engineering (Quick idea)\n",
        "- Overfitting and Underfitting\n",
        "- 7 Step ML workflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_udUG0BS0ow"
      },
      "source": [
        "# The Four Branches of ML\n",
        "## ___Supervised Learning___\n",
        "\n",
        "Although supervised learning mostly consists of __classification__ and __regression__, there\n",
        "are more exotic variants as well, including the following (with examples):\n",
        "\n",
        "- _Sequence generation (seq2seq)_—Given a picture, predict a caption describing it. Sequence\n",
        "generation can sometimes be reformulated as a series of classification problems\n",
        "(such as repeatedly predicting a word or token in a sequence).\n",
        "\n",
        "- _Syntax tree prediction_—Given a sentence, predict its decomposition into a syntax\n",
        "tree.\n",
        "\n",
        "-  _Object detection_—Given a picture, draw a bounding box around certain objects\n",
        "inside the picture. This can also be expressed as a classification problem (given\n",
        "many candidate bounding boxes, classify the contents of each one) or as a joint\n",
        "classification and regression problem, where the bounding-box coordinates are\n",
        "predicted via vector regression.\n",
        "\n",
        "- _Image segmentation_—Given a picture, draw a pixel-level mask on a specific object.\n",
        "\n",
        "\n",
        "\n",
        "## _Unspervised Learning_\n",
        "__Dimensionality reduction__ and __clustering__ are well-known\n",
        "categories of unsupervised learning.\n",
        "\n",
        "This branch of machine learning consists of finding interesting transformations of the\n",
        "input data without the help of any targets, for the purposes of data visualization, data\n",
        "compression, or data denoising, or to better understand the correlations present in\n",
        "the data at hand. Unsupervised learning is the bread and butter of data analytics, and\n",
        "it’s often a necessary step in better understanding a dataset before attempting to solve\n",
        "a supervised-learning problem. \n",
        "## _Self Supervised Learning_\n",
        "Self-supervised learning is supervised learning without human-annotated labels—you can think of it as supervised learning without any humans in the loop. There are still labels involved (because the learning has to be\n",
        "supervised by something), but they’re generated from the input data, typically using a\n",
        "heuristic algorithm.\n",
        "\n",
        "- For instance, __autoencoders__ are a well-known instance of self-supervised learning,\n",
        "where the generated targets are the input, unmodified. \n",
        "\n",
        "- In the same way, trying to predict\n",
        "the next frame in a video, given past frames, or the next word in a text, given previous\n",
        "words, are instances of self-supervised learning (__temporally supervised learning__, in this\n",
        "case: supervision comes from future input data). \n",
        "\n",
        "Note that the distinction between supervised, self-supervised, and unsupervised learning can be blurry sometimes—these\n",
        "categories are more of a continuum without solid borders. \n",
        "\n",
        "Self-supervised learning can\n",
        "be reinterpreted as either supervised or unsupervised learning, depending on whether\n",
        "you pay attention to the learning mechanism or to the context of its application.\n",
        "\n",
        "## _Reinforcement Learning_\n",
        "In reinforcement learning,\n",
        "an __agent__ receives information about its environment and learns to choose actions that\n",
        "will maximize some __reward__. For instance, a neural network that “looks” at a videogame\n",
        "screen and outputs game actions in order to maximize its score can be trained\n",
        "via reinforcement learning.\n",
        "\n",
        "Examples: DeepMind Atari and AlphaGo\n",
        "![04_1_RL.png](https://github.com/ahmadelsallab/practical_dl/blob/master/Keras/notebooks/imgs/04_1_RL\n",
        ".png?raw=true)\n",
        "Currently, reinforcement learning is mostly a research area and hasn’t yet had significant\n",
        "practical successes beyond games. In time, however, we expect to see reinforcement\n",
        "learning take over an increasingly large range of real-world applications:\n",
        "self-driving cars, robotics, resource management, education, and so on. It’s an idea\n",
        "whose time has come, or will come soon.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHsriQ9mS0oz"
      },
      "source": [
        "# ML Glossary:\n",
        "- __Sample or input__—One data point that goes into your model.\n",
        "- __Prediction or output__—What comes out of your model.\n",
        "- __Target__—The truth. What your model should ideally have predicted, according\n",
        "to an external source of data.\n",
        "\n",
        "- __Prediction error or loss value__—A measure of the distance between your\n",
        "model’s prediction and the target.\n",
        "- __Classes__—A set of possible labels to choose from in a classification problem.\n",
        "For example, when classifying cat and dog pictures, “dog” and “cat” are the\n",
        "two classes.\n",
        "- __Label__—A specific instance of a class annotation in a classification problem.\n",
        "For instance, if picture #1234 is annotated as containing the class “dog,”\n",
        "then “dog” is a label of picture #1234.\n",
        "- __Ground-truth or annotations__—All targets for a dataset, typically collected by\n",
        "humans.\n",
        "- __Binary classification__—A classification task where each input sample should\n",
        "be categorized into two exclusive categories.\n",
        "- __Multiclass classification__—A classification task where each input sample\n",
        "should be categorized into more than two categories: for instance, classifying\n",
        "handwritten digits.\n",
        "- __Multilabel classification__—A classification task where each input sample can\n",
        "be assigned multiple labels. For instance, a given image may contain both a\n",
        "cat and a dog and should be annotated both with the “cat” label and the\n",
        "“dog” label. The number of labels per image is usually variable.\n",
        "- __Scalar regression__—A task where the target is a continuous scalar value. Predicting\n",
        "house prices is a good example: the different target prices form a continuous\n",
        "space.\n",
        "- __Vector regression__—A task where the target is a set of continuous values: for\n",
        "example, a continuous vector. If you’re doing regression against multiple values\n",
        "(such as the coordinates of a bounding box in an image), then you’re\n",
        "doing vector regression.\n",
        "- __Mini-batch or batch__—A small set of samples (typically between 8 and 128)\n",
        "that are processed simultaneously by the model. The number of samples is\n",
        "often a power of 2, to facilitate memory allocation on GPU. When training, a\n",
        "mini-batch is used to compute a single gradient-descent update applied to\n",
        "the weights of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bi1Zd-HXS0o3"
      },
      "source": [
        "# Evaluation protocols of ML:\n",
        "we split the data into a __training set__, a __validation set__, and a __test set__.\n",
        "\n",
        "You train on the training data and evaluate your model\n",
        "on the validation data. Once your model is ready for prime time, you test it one final\n",
        "time on the test data.\n",
        "\n",
        "\n",
        "The reason not to evaluate the models on the same data\n",
        "they were trained on quickly became evident: after just a few epochs, all three models\n",
        "began to __overfit__. That is, their performance on never-before-seen data started stalling\n",
        "(or worsening) compared to their performance on the training data—which always\n",
        "improves as training progresses.\n",
        "\n",
        "In machine learning, the goal is to achieve models that __generalize__—that perform\n",
        "well on never-before-seen data—and overfitting is the central obstacle."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64T0QpyPS0o5"
      },
      "source": [
        "## Why validation set?\n",
        "You may ask, why not have two sets: a training set and a test set? You’d train on the\n",
        "training data and evaluate on the test data. Much simpler!\n",
        "The reason is that developing a model always involves tuning its configuration: for\n",
        "example, choosing the number of layers or the size of the layers (called the __hyperparameters__\n",
        "of the model, to distinguish them from the __parameters__, which are the network’s\n",
        "_weights_). You do this tuning by using as a feedback signal the performance of\n",
        "the model on the validation data.\n",
        "\n",
        "\n",
        "\n",
        "### Information leaks to validation set\n",
        "In essence, this tuning is a form of _learning_: a search\n",
        "for a good configuration in some parameter space. As a result, tuning the configuration\n",
        "of the model based on its performance on the validation set can quickly result in\n",
        "overfitting to the validation set, even though your model is never directly trained on it.\n",
        "\n",
        "Central to this phenomenon is the notion of _information leaks_. Every time you tune\n",
        "a hyperparameter of your model based on the model’s performance on the validation\n",
        "set, some information about the validation data leaks into the model. If you do this\n",
        "only once, for one parameter, then very few bits of information will leak, and your validation\n",
        "set will remain reliable to evaluate the model. But if you repeat this many\n",
        "times—running one experiment, evaluating on the validation set, and modifying your\n",
        "model as a result—then you’ll leak an increasingly significant amount of information\n",
        "about the validation set into the model.\n",
        "At the end of the day, you’ll end up with a model that performs artificially well on\n",
        "the validation data, because that’s what you optimized it for. \n",
        "\n",
        "_You care about performance on completely new data, not the validation data, so you need to use a completely\n",
        "different, never-before-seen dataset to evaluate the model: the test dataset._\n",
        "\n",
        "_Your model shouldn’t have had access to any information about the test set, even indirectly._\n",
        "\n",
        "## How to split the data into train, val and test?\n",
        "Splitting your data into training, validation, and test sets may seem straightforward,\n",
        "but there are a few advanced ways to do it that can come in handy when little data is\n",
        "available. Let’s review three classic evaluation recipes: simple hold-out validation, Kfold\n",
        "validation, and iterated K-fold validation with shuffling.\n",
        "\n",
        "### SIMPLE HOLD-OUT VALIDATION\n",
        "\n",
        "Set apart some fraction of your data as your test set. Train on the remaining data, and\n",
        "evaluate on the test set. As you saw in the previous sections, in order to prevent information\n",
        "leaks, you shouldn’t tune your model based on the test set, and therefore you\n",
        "should also reserve a validation set.\n",
        "\n",
        "![04_2_Val_Hold_Out.png](https://github.com/ahmadelsallab/practical_dl/blob/master/Keras/notebooks/imgs/04_2_Val_Hold_Out.png?raw=true)\n",
        "\n",
        "This is the simplest evaluation protocol, and it suffers from one flaw: if little data is\n",
        "available, then your validation and test sets may contain too few samples to be statistically\n",
        "representative of the data at hand.\n",
        "\n",
        "__Few samples is a relative term. So how to know ?__ This is easy to recognize: if different random\n",
        "shuffling rounds of the data before splitting end up yielding very different measures\n",
        "of model performance, then you’re having this issue. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZN_PLwLzX1A9"
      },
      "source": [
        "```\n",
        "num_validation_samples = 10000\n",
        "np.random.shuffle(data)\n",
        "validation_data = data[:num_validation_samples]\n",
        "training_data = data[num_validation_samples:]\n",
        "model = get_model()\n",
        "model.train(training_data)\n",
        "validation_score = model.evaluate(validation_data)\n",
        "# At this point you can tune your model,\n",
        "# retrain it, evaluate it, tune it again...\n",
        "model = get_model()\n",
        "model.train(np.concatenate([training_data, validation_data]))\n",
        "test_score = model.evaluate(test_data)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADr1Y4iKS0pI"
      },
      "source": [
        "### K-FOLD VALIDATION\n",
        "\n",
        "With this approach, you split your data into K partitions of equal size. For each partition\n",
        "i, train a model on the remaining K – 1 partitions, and evaluate it on partition i.\n",
        "Your final score is then the averages of the K scores obtained. This method is helpful\n",
        "when the performance of your model shows significant variance based on your traintest\n",
        "split. Like hold-out validation, this method doesn’t exempt you from using a distinct\n",
        "validation set for model calibration.\n",
        "\n",
        "![04_3_K_Folds.png](https://github.com/ahmadelsallab/practical_dl/blob/master/Keras/notebooks/imgs/04_3_K_Folds.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGZSwCE_Yj3Q"
      },
      "source": [
        "\n",
        "```\n",
        "\n",
        "k = 4\n",
        "num_validation_samples = len(data) // k\n",
        "np.random.shuffle(data)\n",
        "validation_scores = []\n",
        "for fold in range(k):\n",
        "    validation_data = data[num_validation_samples * fold:\n",
        "    num_validation_samples * (fold + 1)]\n",
        "    training_data = data[:num_validation_samples * fold] +\n",
        "    data[num_validation_samples * (fold + 1):]\n",
        "    model = get_model()\n",
        "    model.train(training_data)\n",
        "    validation_score = model.evaluate(validation_data)\n",
        "    validation_scores.append(validation_score)\n",
        "\n",
        "final_score = np.mean(validation_scores) \n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSY8gft9S0pR"
      },
      "source": [
        "__When to tune the model__:\n",
        "\n",
        "1- After each fold training: fast conversion, high oscillation. Not preferred unless you have very very little data.\n",
        "\n",
        "2- After all folds training: slow conversion, stable. This is preferred.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VJpsHi5S0pT"
      },
      "source": [
        "### ITERATED K-FOLD VALIDATION WITH SHUFFLING \n",
        "\n",
        "This one is for situations in which you have relatively little data available and you need\n",
        "to evaluate your model as precisely as possible. It consists of applying K-fold validation multiple times, shuffling\n",
        "the data every time before splitting it K ways. The final score is the average of the\n",
        "scores obtained at each run of K-fold validation. Note that you end up training and\n",
        "evaluating P × K models (where P is the number of iterations you use), which can very\n",
        "expensive.\n",
        "```\n",
        "k = 4\n",
        "p = 3\n",
        "validation_scores = []\n",
        "for iteration in range(p):\n",
        "  num_validation_samples = len(data) // k\n",
        "  np.random.shuffle(data)\n",
        "\n",
        "  for fold in range(k):\n",
        "      validation_data = data[num_validation_samples * fold:\n",
        "      num_validation_samples * (fold + 1)]\n",
        "      training_data = data[:num_validation_samples * fold] +\n",
        "      data[num_validation_samples * (fold + 1):]\n",
        "      model = get_model()\n",
        "      model.train(training_data)\n",
        "      validation_score = model.evaluate(validation_data)\n",
        "      validation_scores.append(validation_score)\n",
        "final_score = np.mean(validation_scores)\n",
        "```   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8CvbEGwS0pV"
      },
      "source": [
        "## Things to keep in mind\n",
        "\n",
        "Keep an eye out for the following when you’re choosing an evaluation protocol:\n",
        "-  ___Data representativeness___—You want both your training set and test set to be representative\n",
        "of the data at hand. For instance, if you’re trying to classify images of\n",
        "digits, and you’re starting from an array of samples where the samples are\n",
        "ordered by their class, taking the first 80% of the array as your training set and\n",
        "the remaining 20% as your test set will result in your training set containing\n",
        "only classes 0–7, whereas your test set contains only classes 8–9. This seems like\n",
        "a ridiculous mistake, but it’s surprisingly common. For this reason, you usually\n",
        "should randomly shuffle your data before splitting it into training and test sets.\n",
        "- ___The arrow of time___—If you’re trying to predict the future given the past (for example,\n",
        "tomorrow’s weather, stock movements, and so on), you should not randomly\n",
        "shuffle your data before splitting it, because doing so will create a\n",
        "temporal leak: your model will effectively be trained on data from the future. In\n",
        "such situations, you should always make sure all data in your test set is posterior\n",
        "to the data in the training set.\n",
        "\n",
        "\n",
        "\n",
        "- ___Redundancy___ in your data—If some data points in your data appear twice (fairly\n",
        "common with real-world data), then shuffling the data and splitting it into a\n",
        "training set and a validation set will result in redundancy between the training\n",
        "and validation sets. In effect, you’ll be testing on part of your training data,\n",
        "which is the worst thing you can do! Make sure your training set and validation\n",
        "set are disjoint.\n",
        "\n",
        "Same scenario results if you have temporal data (e.g. videos), and you shuffle and train/val split, you end up having correlated samples from train in val."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMFxBmjjhitl"
      },
      "source": [
        "# All tha above is just for evaluation!\n",
        "\n",
        "__Which model to use?__\n",
        "\n",
        "It means, after you decide on the best hyper parameters, you are allowed to train on all the train data (regardless of val split methods like K-Folds,...etc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYYkMU99S0pX"
      },
      "source": [
        "# Data preprocessing for neural networks\n",
        "\n",
        "In general, the data pipeline includes:\n",
        "1. _Loading data_: This includes reading the data files, exploring and visualizing the samples, exploring the labels distribution, ...etc\n",
        "2. _Preprocessing_: This This includes vectorization, normalization, handling missing values, and feature extraction (mostly ML). \n",
        "\n",
        "## Vectorization (Binarization)\n",
        "All inputs and targets in a neural network must be tensors of floating-point data (or, in\n",
        "specific cases, tensors of integers). Whatever data you need to process—sound,\n",
        "images, text—you must first turn into tensors, a step called ___data vectorization___. For\n",
        "instance, in the two previous text-classification examples, we started from text represented\n",
        "as lists of integers (standing for sequences of words), and we used one-hot\n",
        "encoding to turn them into a tensor of float32 data. In the examples of classifying\n",
        "digits and predicting house prices, the data already came in vectorized form, so you\n",
        "were able to skip this step.\n",
        "\n",
        "## Normalization\n",
        "In general, it isn’t safe to feed into a neural network data that takes relatively large values\n",
        "(for example, multidigit integers, which are much larger than the initial values taken\n",
        "by the weights of a network) or data that is heterogeneous (for example, data where one\n",
        "feature is in the range 0–1 and another is in the range 100–200). Doing so can trigger\n",
        "large gradient updates that will prevent the network from converging.\n",
        "\n",
        "To make learning easier for your network, your data should have the following characteristics:\n",
        "\n",
        "__Take small values: Same Range__\n",
        "Typically, most values should be in the 0–1 range.\n",
        "\n",
        "In the digit-classification example, you started from image data encoded as integers in\n",
        "the 0–255 range, encoding grayscale values. Before you fed this data into your network,\n",
        "you had to cast it to float32 and divide by 255 so you’d end up with floatingpoint\n",
        "values in the 0–1 range. \n",
        "\n",
        "__Homogeneous: Same scale__\n",
        "That is, all features should take values in roughly the same\n",
        "range.\n",
        "\n",
        "Similarly, when predicting house prices, you started\n",
        "from features that took a variety of ranges—some features had small floating-point values,\n",
        "others had fairly large integer values. \n",
        "\n",
        "__Z-Normalization__\n",
        "Before you fed this data into your network, you had to normalize each feature independently so that it had a standard deviation\n",
        "of 1 and a mean of 0.\n",
        "\n",
        "The following stricter normalization practice is common and can help,\n",
        "although it isn’t always necessary (for example, you didn’t do this in the digit-classification\n",
        "example, _because all the values where homogenous, so only scaling was enough_):\n",
        "-  Normalize each feature independently to have a mean of 0.\n",
        "-  Normalize each feature independently to have a standard deviation of 1.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGe3uMYJZy7D"
      },
      "source": [
        "```\n",
        "x -= x.mean(axis=0)\n",
        "x /= x.std(axis=0)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gl-RrvJxS0pd"
      },
      "source": [
        "## Handling missing data\n",
        "You may sometimes have missing values in your data. For instance, in the house-price example, the first feature (the column of index 0 in the data) was the per capita crime rate. What if this feature wasn’t available for all samples? You’d then have missing values in the training or test data.\n",
        "\n",
        "__Handle missing data as 0's__\n",
        "\n",
        "In general, with neural networks, it’s safe to input missing values as 0, ___with the condition\n",
        "that 0 isn’t already a meaningful value___. The network will learn from exposure to\n",
        "the data that the value 0 means missing data and will start ignoring the value.\n",
        "\n",
        "__Artificial missing data: Data augmentation__\n",
        "\n",
        "Note that if you’re expecting missing values in the test data, but the network was\n",
        "trained on data without any missing values, the network won’t have learned to ignore\n",
        "missing values! \n",
        "\n",
        "In this situation, you should artificially generate training samples with\n",
        "missing entries: copy some training samples several times, and drop some of the features\n",
        "that you expect are likely to be missing in the test data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmt4Ab1rS0pe"
      },
      "source": [
        "\n",
        "## Features engineering (mostly ML)\n",
        "\n",
        "Feature engineering is the process of using your own knowledge about the data and about the machine-learning algorithm at hand (in this case, a neural network) to make the algorithm work better by applying ___hardcoded (nonlearned) transformations___ to the data before it goes into the model.\n",
        "## The power of representation\n",
        "We have two categories (blue/green) which we want to cluster/separate/classify\n",
        "So separating the two classes in this case is much easier just by using polar representation\n",
        "\n",
        "![04_4_Coordinates.png](https://github.com/ahmadelsallab/practical_dl/blob/master/Keras/notebooks/imgs/04_4_Coordinates.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPBu_2rYS0pf"
      },
      "source": [
        "![04_5_Time.png](https://github.com/ahmadelsallab/practical_dl/blob/master/Keras/notebooks/imgs/04_5_Time.png?raw=true)\n",
        "\n",
        "Although it is possible to learn the time from pixels using ConvNets, but it's computationally expensive as well. If you can represent the time in better way, like positions of hands, or even better, the angles, then the problem is much easier for a network, or even a simple classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMNp1LUQS0pg"
      },
      "source": [
        "## Before Deep Learning\n",
        "Before deep learning, feature engineering used to be critical, ___because classical\n",
        "shallow algorithms didn’t have hypothesis spaces rich enough to learn useful features\n",
        "by themselves___. \n",
        "\n",
        "The way you presented the data to the algorithm was essential to its success.\n",
        "For instance, before convolutional neural networks became successful on the\n",
        "MNIST digit-classification problem, solutions were typically based on hardcoded features\n",
        "such as the number of loops in a digit image, the height of each digit in an\n",
        "image, a histogram of pixel values, and so on.\n",
        "\n",
        "## Deep Learning for rescue\n",
        "Fortunately, modern deep learning removes the need for most feature engineering,\n",
        "because neural networks are capable of automatically extracting useful features\n",
        "from raw data. \n",
        "\n",
        "## Does this mean you don’t have to worry about feature engineering as long as you’re using deep neural networks?\n",
        "\n",
        "_No, for two reasons:_\n",
        "\n",
        "- Good features still allow you to solve problems more elegantly while using ___fewer resources___. For instance, it would be ridiculous to solve the problem of reading a\n",
        "clock face using a convolutional neural network.\n",
        "\n",
        "- Good features let you solve a problem with ___far less data___. The ability of deeplearning\n",
        "models to learn features on their own relies on having lots of training data available; if you have only a few samples, then the information value in their features becomes critical."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5RSLUg0S0ph"
      },
      "source": [
        "# Overfitting and underfitting\n",
        "\n",
        "The fundamental issue in machine learning is the tension between _optimization and generalization_.\n",
        "_Optimization_ refers to the process of adjusting a model to get the best performance possible on the training data (the learning in machine learning),\n",
        "\n",
        "Whereas _generalization_ refers to how well the trained model performs on data it has\n",
        "never seen before. \n",
        "\n",
        "_The goal of the game is to get good generalization, of course, but\n",
        "you don’t control generalization; you can only adjust the model based on its training\n",
        "data._\n",
        "\n",
        "\n",
        "__Your first problem is underfitting__\n",
        "At the beginning of training, optimization and generalization are correlated: the\n",
        "lower the loss on training data, the lower the loss on test data. While this is happening,\n",
        "your model is said to be __underfit__: there is still progress to be made; the network hasn’t\n",
        "yet modeled all relevant patterns in the training data. \n",
        "\n",
        "But after a certain number of iterations on the training data, generalization stops improving, and validation metrics\n",
        "stall and then begin to degrade: the model is starting to overfit. That is, it’s beginning to learn patterns that are specific to the training data but that are misleading or irrelevant when it comes to new data.\n",
        "\n",
        "__What if you do not overfit even the training data?__\n",
        "Then you have a fundamental issue. The only solution is to use a bigger network to give more capactiy to represent the data.\n",
        "That's why we always start with a big model, and then treat overfitting:\n",
        "\n",
        "Copied from fast.ai course\n",
        "![04_6_5_steps_to_avoid_overfitting.png](https://github.com/ahmadelsallab/practical_dl/blob/master/Keras/notebooks/imgs/04_6_5_steps_to_avoid_overfitting.png?raw=true)\n",
        "\n",
        "As you can see, the last resort is the reduce the network size, we have several things to try first:\n",
        "- _Increasing the data_ is the best solution, but not always possible\n",
        "- _Data augmnentation_ is a form of _smartly_ or _artificially_ increasing the data, by applying reasonable transfromations to the already labeled data we have. \n",
        "- _Generalizable architecture_ is an advanced topic related to transfer learning, where we start from pre-trained model.\n",
        "- _Regularization_ The processing of fighting overfitting in this way is called _regularization_. There are many techniques. L2 Regularization adds a penality on the growth of the network capacity, _Dropout_ also is a good cure, where we stop some weights from learning occasionally.\n",
        "- _Reduce model complexity_ by using smaller model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMIkcA2ZS0pj"
      },
      "source": [
        "### Let's apply to the movie classification example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slQSGlH5S0pk"
      },
      "source": [
        "from keras.datasets import imdb\n",
        "import numpy as np\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
        "\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "    # Create an all-zero matrix of shape (len(sequences), dimension)\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        results[i, sequence] = 1.  # set specific indices of results[i] to 1s\n",
        "    return results\n",
        "\n",
        "# Our vectorized training data\n",
        "x_train = vectorize_sequences(train_data)\n",
        "# Our vectorized test data\n",
        "x_test = vectorize_sequences(test_data)\n",
        "# Our vectorized labels\n",
        "y_train = np.asarray(train_labels).astype('float32')\n",
        "y_test = np.asarray(test_labels).astype('float32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kP4ZymIMS0pn"
      },
      "source": [
        "## Start with a big model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KddG5SfKS0pp"
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "original_model = models.Sequential()\n",
        "original_model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
        "original_model.add(layers.Dense(16, activation='relu'))\n",
        "original_model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "original_model.compile(optimizer='rmsprop',\n",
        "                       loss='binary_crossentropy',\n",
        "                       metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6I4bfvDaKDc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "outputId": "f36e3249-ec72-4185-9cfe-a07d97bd77b1"
      },
      "source": [
        "original_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_16 (Dense)             (None, 16)                160016    \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 160,305\n",
            "Trainable params: 160,305\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38M0ctDbS0pq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        },
        "outputId": "b42b800c-32b9-414e-c1f8-ee5abfcd4f42"
      },
      "source": [
        "original_hist = original_model.fit(x_train, y_train,\n",
        "                                   epochs=20,\n",
        "                                   batch_size=512,\n",
        "                                   validation_data=(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/20\n",
            "25000/25000 [==============================] - 5s 189us/step - loss: 0.4484 - acc: 0.8173 - val_loss: 0.3367 - val_acc: 0.8768\n",
            "Epoch 2/20\n",
            "25000/25000 [==============================] - 4s 158us/step - loss: 0.2547 - acc: 0.9100 - val_loss: 0.3178 - val_acc: 0.8702\n",
            "Epoch 3/20\n",
            "25000/25000 [==============================] - 4s 155us/step - loss: 0.1967 - acc: 0.9297 - val_loss: 0.2822 - val_acc: 0.8882\n",
            "Epoch 4/20\n",
            "25000/25000 [==============================] - 4s 155us/step - loss: 0.1669 - acc: 0.9412 - val_loss: 0.2950 - val_acc: 0.8837\n",
            "Epoch 5/20\n",
            "25000/25000 [==============================] - 4s 153us/step - loss: 0.1432 - acc: 0.9490 - val_loss: 0.3213 - val_acc: 0.8782\n",
            "Epoch 6/20\n",
            "25000/25000 [==============================] - 4s 154us/step - loss: 0.1269 - acc: 0.9556 - val_loss: 0.3462 - val_acc: 0.8732\n",
            "Epoch 7/20\n",
            "25000/25000 [==============================] - 4s 153us/step - loss: 0.1128 - acc: 0.9609 - val_loss: 0.3609 - val_acc: 0.8712\n",
            "Epoch 8/20\n",
            "25000/25000 [==============================] - 4s 154us/step - loss: 0.1011 - acc: 0.9650 - val_loss: 0.4079 - val_acc: 0.8640\n",
            "Epoch 9/20\n",
            "25000/25000 [==============================] - 4s 154us/step - loss: 0.0905 - acc: 0.9686 - val_loss: 0.4183 - val_acc: 0.8646\n",
            "Epoch 10/20\n",
            "25000/25000 [==============================] - 4s 155us/step - loss: 0.0812 - acc: 0.9716 - val_loss: 0.4467 - val_acc: 0.8622\n",
            "Epoch 11/20\n",
            "25000/25000 [==============================] - 4s 154us/step - loss: 0.0737 - acc: 0.9756 - val_loss: 0.4724 - val_acc: 0.8622\n",
            "Epoch 12/20\n",
            "25000/25000 [==============================] - 4s 155us/step - loss: 0.0631 - acc: 0.9794 - val_loss: 0.5491 - val_acc: 0.8530\n",
            "Epoch 13/20\n",
            "25000/25000 [==============================] - 4s 155us/step - loss: 0.0595 - acc: 0.9799 - val_loss: 0.5326 - val_acc: 0.8584\n",
            "Epoch 14/20\n",
            "25000/25000 [==============================] - 4s 155us/step - loss: 0.0514 - acc: 0.9832 - val_loss: 0.5671 - val_acc: 0.8559\n",
            "Epoch 15/20\n",
            "25000/25000 [==============================] - 4s 157us/step - loss: 0.0473 - acc: 0.9854 - val_loss: 0.6062 - val_acc: 0.8540\n",
            "Epoch 16/20\n",
            "25000/25000 [==============================] - 4s 155us/step - loss: 0.0391 - acc: 0.9886 - val_loss: 0.6949 - val_acc: 0.8453\n",
            "Epoch 17/20\n",
            "25000/25000 [==============================] - 4s 156us/step - loss: 0.0383 - acc: 0.9870 - val_loss: 0.6774 - val_acc: 0.8514\n",
            "Epoch 18/20\n",
            "25000/25000 [==============================] - 4s 156us/step - loss: 0.0298 - acc: 0.9915 - val_loss: 0.7065 - val_acc: 0.8510\n",
            "Epoch 19/20\n",
            "25000/25000 [==============================] - 4s 155us/step - loss: 0.0281 - acc: 0.9918 - val_loss: 0.7553 - val_acc: 0.8478\n",
            "Epoch 20/20\n",
            "25000/25000 [==============================] - 4s 156us/step - loss: 0.0248 - acc: 0.9929 - val_loss: 0.7779 - val_acc: 0.8494\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_kjHRfES0pt"
      },
      "source": [
        "Here's a comparison of the validation losses of the original network and the smaller network. The dots are the validation loss values of \n",
        "the smaller network, and the crosses are the initial network (remember: a lower validation loss signals a better model)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkVfMuppS0pu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "outputId": "ce4b8e31-ad4e-41c1-a8fd-357345f9dc04"
      },
      "source": [
        "smaller_model = models.Sequential()\n",
        "smaller_model.add(layers.Dense(4, activation='relu', input_shape=(10000,)))\n",
        "smaller_model.add(layers.Dense(4, activation='relu'))\n",
        "smaller_model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "smaller_model.compile(optimizer='rmsprop',\n",
        "                      loss='binary_crossentropy',\n",
        "                      metrics=['acc'])\n",
        "smaller_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_19 (Dense)             (None, 4)                 40004     \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 4)                 20        \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 1)                 5         \n",
            "=================================================================\n",
            "Total params: 40,029\n",
            "Trainable params: 40,029\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiQc6lNAS0pw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        },
        "outputId": "1fa84f31-f752-4972-ab4a-67103eaa0bc0"
      },
      "source": [
        "smaller_model_hist = smaller_model.fit(x_train, y_train,\n",
        "                                       epochs=20,\n",
        "                                       batch_size=512,\n",
        "                                       validation_data=(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/20\n",
            "25000/25000 [==============================] - 4s 160us/step - loss: 0.5235 - acc: 0.8072 - val_loss: 0.4188 - val_acc: 0.8674\n",
            "Epoch 2/20\n",
            "25000/25000 [==============================] - 3s 136us/step - loss: 0.3373 - acc: 0.8964 - val_loss: 0.3343 - val_acc: 0.8815\n",
            "Epoch 3/20\n",
            "25000/25000 [==============================] - 3s 135us/step - loss: 0.2585 - acc: 0.9164 - val_loss: 0.2935 - val_acc: 0.8895\n",
            "Epoch 4/20\n",
            "25000/25000 [==============================] - 3s 135us/step - loss: 0.2155 - acc: 0.9286 - val_loss: 0.2811 - val_acc: 0.8893\n",
            "Epoch 5/20\n",
            "25000/25000 [==============================] - 3s 135us/step - loss: 0.1867 - acc: 0.9376 - val_loss: 0.2871 - val_acc: 0.8843\n",
            "Epoch 6/20\n",
            "25000/25000 [==============================] - 3s 136us/step - loss: 0.1665 - acc: 0.9430 - val_loss: 0.2832 - val_acc: 0.8858\n",
            "Epoch 7/20\n",
            "25000/25000 [==============================] - 3s 136us/step - loss: 0.1506 - acc: 0.9500 - val_loss: 0.2902 - val_acc: 0.8844\n",
            "Epoch 8/20\n",
            "25000/25000 [==============================] - 3s 136us/step - loss: 0.1370 - acc: 0.9548 - val_loss: 0.3008 - val_acc: 0.8825\n",
            "Epoch 9/20\n",
            "25000/25000 [==============================] - 3s 135us/step - loss: 0.1260 - acc: 0.9579 - val_loss: 0.3120 - val_acc: 0.8795\n",
            "Epoch 10/20\n",
            "25000/25000 [==============================] - 3s 135us/step - loss: 0.1164 - acc: 0.9620 - val_loss: 0.3364 - val_acc: 0.8734\n",
            "Epoch 11/20\n",
            "25000/25000 [==============================] - 3s 136us/step - loss: 0.1081 - acc: 0.9655 - val_loss: 0.3392 - val_acc: 0.8758\n",
            "Epoch 12/20\n",
            "25000/25000 [==============================] - 3s 135us/step - loss: 0.1001 - acc: 0.9690 - val_loss: 0.3562 - val_acc: 0.8718\n",
            "Epoch 13/20\n",
            "25000/25000 [==============================] - 3s 136us/step - loss: 0.0926 - acc: 0.9723 - val_loss: 0.3687 - val_acc: 0.8715\n",
            "Epoch 14/20\n",
            "25000/25000 [==============================] - 3s 136us/step - loss: 0.0869 - acc: 0.9739 - val_loss: 0.3872 - val_acc: 0.8699\n",
            "Epoch 15/20\n",
            "25000/25000 [==============================] - 3s 137us/step - loss: 0.0806 - acc: 0.9758 - val_loss: 0.4036 - val_acc: 0.8667\n",
            "Epoch 16/20\n",
            "25000/25000 [==============================] - 3s 136us/step - loss: 0.0751 - acc: 0.9782 - val_loss: 0.4194 - val_acc: 0.8653\n",
            "Epoch 17/20\n",
            "25000/25000 [==============================] - 3s 136us/step - loss: 0.0693 - acc: 0.9804 - val_loss: 0.4377 - val_acc: 0.8648\n",
            "Epoch 18/20\n",
            "25000/25000 [==============================] - 3s 135us/step - loss: 0.0643 - acc: 0.9825 - val_loss: 0.4567 - val_acc: 0.8636\n",
            "Epoch 19/20\n",
            "25000/25000 [==============================] - 3s 135us/step - loss: 0.0596 - acc: 0.9844 - val_loss: 0.4776 - val_acc: 0.8618\n",
            "Epoch 20/20\n",
            "25000/25000 [==============================] - 3s 136us/step - loss: 0.0553 - acc: 0.9850 - val_loss: 0.4982 - val_acc: 0.8592\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c81vyasyS0p1"
      },
      "source": [
        "epochs = range(1, 21)\n",
        "original_val_loss = original_hist.history['val_loss']\n",
        "smaller_model_val_loss = smaller_model_hist.history['val_loss']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hS23SAPeS0p5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "05b1469b-ccb4-40ba-c134-8727226dcef0"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# b+ is for \"blue cross\"\n",
        "plt.plot(epochs, original_val_loss, 'ro', label='Original model')\n",
        "# \"bo\" is for \"blue dot\"\n",
        "plt.plot(epochs, smaller_model_val_loss, 'bo', label='Smaller model')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFaCAYAAAA3jtULAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRuEtIiJiMQpvERERi1F4i4iIWMz/B7FzIAXuF7GyAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsSxKacpaFD1"
      },
      "source": [
        "\n",
        "As you can see, the smaller network starts overfitting later than the reference one (after 6 epochs rather than 4) and its performance \n",
        "degrades much more slowly once it starts overfitting.\n",
        "\n",
        "Now, for kicks, let's add to this benchmark a network that has much more capacity, far more than the problem would warrant:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5ajFm8WS0p-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "outputId": "97e2d274-017b-48f1-cc27-362aedeb0eff"
      },
      "source": [
        "bigger_model = models.Sequential()\n",
        "bigger_model.add(layers.Dense(512, activation='relu', input_shape=(10000,)))\n",
        "bigger_model.add(layers.Dense(512, activation='relu'))\n",
        "bigger_model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "bigger_model.compile(optimizer='rmsprop',\n",
        "                     loss='binary_crossentropy',\n",
        "                     metrics=['acc'])\n",
        "bigger_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_22 (Dense)             (None, 512)               5120512   \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 1)                 513       \n",
            "=================================================================\n",
            "Total params: 5,383,681\n",
            "Trainable params: 5,383,681\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68WxRW3qS0qB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        },
        "outputId": "afd062f7-d4ac-44dd-eb65-1c961a0579fa"
      },
      "source": [
        "bigger_model_hist = bigger_model.fit(x_train, y_train,\n",
        "                                     epochs=20,\n",
        "                                     batch_size=512,\n",
        "                                     validation_data=(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/20\n",
            "25000/25000 [==============================] - 20s 787us/step - loss: 0.4648 - acc: 0.7958 - val_loss: 0.2823 - val_acc: 0.8889\n",
            "Epoch 2/20\n",
            "25000/25000 [==============================] - 18s 740us/step - loss: 0.2234 - acc: 0.9120 - val_loss: 0.2930 - val_acc: 0.8782\n",
            "Epoch 3/20\n",
            "25000/25000 [==============================] - 19s 741us/step - loss: 0.1350 - acc: 0.9508 - val_loss: 0.3700 - val_acc: 0.8646\n",
            "Epoch 4/20\n",
            "25000/25000 [==============================] - 18s 739us/step - loss: 0.0681 - acc: 0.9804 - val_loss: 0.3790 - val_acc: 0.8842\n",
            "Epoch 5/20\n",
            "25000/25000 [==============================] - 19s 741us/step - loss: 0.0693 - acc: 0.9868 - val_loss: 0.4554 - val_acc: 0.8805\n",
            "Epoch 6/20\n",
            "25000/25000 [==============================] - 19s 742us/step - loss: 0.0025 - acc: 0.9997 - val_loss: 0.6068 - val_acc: 0.8794\n",
            "Epoch 7/20\n",
            "25000/25000 [==============================] - 19s 741us/step - loss: 0.0956 - acc: 0.9900 - val_loss: 0.6480 - val_acc: 0.8785\n",
            "Epoch 8/20\n",
            "25000/25000 [==============================] - 19s 747us/step - loss: 8.1772e-04 - acc: 0.9999 - val_loss: 0.7104 - val_acc: 0.8782\n",
            "Epoch 9/20\n",
            "25000/25000 [==============================] - 19s 760us/step - loss: 8.6140e-05 - acc: 1.0000 - val_loss: 0.8144 - val_acc: 0.8779\n",
            "Epoch 10/20\n",
            "25000/25000 [==============================] - 19s 743us/step - loss: 9.6032e-06 - acc: 1.0000 - val_loss: 0.9157 - val_acc: 0.8768\n",
            "Epoch 11/20\n",
            "25000/25000 [==============================] - 18s 740us/step - loss: 1.1021e-06 - acc: 1.0000 - val_loss: 1.0716 - val_acc: 0.8710\n",
            "Epoch 12/20\n",
            "25000/25000 [==============================] - 19s 741us/step - loss: 0.1416 - acc: 0.9890 - val_loss: 0.9643 - val_acc: 0.8716\n",
            "Epoch 13/20\n",
            "25000/25000 [==============================] - 19s 740us/step - loss: 0.0955 - acc: 0.9887 - val_loss: 0.8555 - val_acc: 0.8738\n",
            "Epoch 14/20\n",
            "25000/25000 [==============================] - 19s 743us/step - loss: 2.1548e-05 - acc: 1.0000 - val_loss: 0.8658 - val_acc: 0.8748\n",
            "Epoch 15/20\n",
            "25000/25000 [==============================] - 19s 740us/step - loss: 6.5766e-06 - acc: 1.0000 - val_loss: 0.9177 - val_acc: 0.8752\n",
            "Epoch 16/20\n",
            "25000/25000 [==============================] - 19s 740us/step - loss: 2.1278e-06 - acc: 1.0000 - val_loss: 0.9876 - val_acc: 0.8761\n",
            "Epoch 17/20\n",
            "25000/25000 [==============================] - 19s 748us/step - loss: 5.2463e-07 - acc: 1.0000 - val_loss: 1.0599 - val_acc: 0.8769\n",
            "Epoch 18/20\n",
            "25000/25000 [==============================] - 19s 748us/step - loss: 1.8514e-07 - acc: 1.0000 - val_loss: 1.1019 - val_acc: 0.8762\n",
            "Epoch 19/20\n",
            "25000/25000 [==============================] - 19s 741us/step - loss: 1.2885e-07 - acc: 1.0000 - val_loss: 1.1310 - val_acc: 0.8764\n",
            "Epoch 20/20\n",
            "25000/25000 [==============================] - 18s 737us/step - loss: 1.1754e-07 - acc: 1.0000 - val_loss: 1.1412 - val_acc: 0.8764\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0qsPrJQS0qG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "outputId": "9d419ae3-75d6-4255-8324-6bab695bff84"
      },
      "source": [
        "bigger_model_val_loss = bigger_model_hist.history['val_loss']\n",
        "\n",
        "plt.plot(epochs, original_val_loss, 'ro', label='Original model')\n",
        "plt.plot(epochs, bigger_model_val_loss, 'bo', label='Bigger model')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFYCAYAAAB6RnQAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdXAe_0MS0qJ"
      },
      "source": [
        "\n",
        "The bigger network starts overfitting almost right away, after just one epoch, and overfits much more severely. Its validation loss is also \n",
        "more noisy.\n",
        "\n",
        "Meanwhile, here are the training losses for our two networks:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZG5Z7_-S0qK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "outputId": "f47978d2-a0ab-42c6-8a4e-71fd3aa10771"
      },
      "source": [
        "original_train_loss = original_hist.history['loss']\n",
        "bigger_model_train_loss = bigger_model_hist.history['loss']\n",
        "\n",
        "plt.plot(epochs, original_train_loss, 'ro', label='Original model')\n",
        "plt.plot(epochs, bigger_model_train_loss, 'bo', label='Bigger model')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Training loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFYCAYAAAB6RnQAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8/+a1KHdQbzj/gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-OGTkPRS0qN"
      },
      "source": [
        "As you can see, the bigger network gets its training loss near zero very quickly. The more capacity the network has, the quicker it will be \n",
        "able to model the training data (resulting in a low training loss), but the more susceptible it is to overfitting (resulting in a large \n",
        "difference between the training and validation loss)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBpvOYvhS0qO"
      },
      "source": [
        "## Adding weight regularization\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "A \"simple model\" in this context is a model where the distribution of parameter values has less entropy (or a model with fewer \n",
        "parameters altogether, as we saw in the section above). Thus a common way to mitigate overfitting is to put constraints on the complexity \n",
        "of a network by forcing its weights to only take small values, which makes the distribution of weight values more \"regular\". This is called \n",
        "\"weight regularization\", and it is done by adding to the loss function of the network a _cost_ associated with having large weights. This \n",
        "cost comes in two flavors:\n",
        "\n",
        "* L1 regularization, where the cost added is proportional to the _absolute value of the weights coefficients_ (i.e. to what is called the \n",
        "\"L1 norm\" of the weights).\n",
        "* L2 regularization, where the cost added is proportional to the _square of the value of the weights coefficients_ (i.e. to what is called \n",
        "the \"L2 norm\" of the weights). L2 regularization is also called _weight decay_ in the context of neural networks. Don't let the different \n",
        "name confuse you: weight decay is mathematically the exact same as L2 regularization.\n",
        "\n",
        "In Keras, weight regularization is added by passing _weight regularizer instances_ to layers as keyword arguments. Let's add L2 weight \n",
        "regularization to our movie review classification network:\n",
        "\n",
        "__Note__ here we follow the normal flow, as we start from the big model, and not the small one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3T9DQFjS0qP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "outputId": "60a11aef-71aa-461f-a5ed-6a72b93f2e5e"
      },
      "source": [
        "from keras import regularizers\n",
        "\n",
        "l2_model = models.Sequential()\n",
        "l2_model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001),\n",
        "                          activation='relu', input_shape=(10000,)))\n",
        "l2_model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001),\n",
        "                          activation='relu'))\n",
        "l2_model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "l2_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_25 (Dense)             (None, 16)                160016    \n",
            "_________________________________________________________________\n",
            "dense_26 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dense_27 (Dense)             (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 160,305\n",
            "Trainable params: 160,305\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWyKuyecS0qR"
      },
      "source": [
        "l2_model.compile(optimizer='rmsprop',\n",
        "                 loss='binary_crossentropy',\n",
        "                 metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtxaW3HsS0qS"
      },
      "source": [
        "`l2(0.001)` means that every coefficient in the weight matrix of the layer will add `0.001 * weight_coefficient_value` to the total loss of \n",
        "the network. Note that because this penalty is _only added at training time_, the loss for this network will be much higher at training \n",
        "than at test time.\n",
        "\n",
        "Here's the impact of our L2 regularization penalty:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQ8ebOZLS0qT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        },
        "outputId": "29a0e752-2d1f-4cec-e329-996ae291faa5"
      },
      "source": [
        "l2_model_hist = l2_model.fit(x_train, y_train,\n",
        "                             epochs=20,\n",
        "                             batch_size=512,\n",
        "                             validation_data=(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/20\n",
            "25000/25000 [==============================] - 5s 190us/step - loss: 0.4877 - acc: 0.8167 - val_loss: 0.3889 - val_acc: 0.8654\n",
            "Epoch 2/20\n",
            "25000/25000 [==============================] - 4s 160us/step - loss: 0.3098 - acc: 0.9059 - val_loss: 0.3303 - val_acc: 0.8889\n",
            "Epoch 3/20\n",
            "25000/25000 [==============================] - 4s 159us/step - loss: 0.2650 - acc: 0.9203 - val_loss: 0.3314 - val_acc: 0.8871\n",
            "Epoch 4/20\n",
            "25000/25000 [==============================] - 4s 159us/step - loss: 0.2457 - acc: 0.9290 - val_loss: 0.3414 - val_acc: 0.8824\n",
            "Epoch 5/20\n",
            "25000/25000 [==============================] - 4s 160us/step - loss: 0.2321 - acc: 0.9348 - val_loss: 0.3801 - val_acc: 0.8687\n",
            "Epoch 6/20\n",
            "25000/25000 [==============================] - 4s 159us/step - loss: 0.2236 - acc: 0.9391 - val_loss: 0.3666 - val_acc: 0.8754\n",
            "Epoch 7/20\n",
            "25000/25000 [==============================] - 4s 159us/step - loss: 0.2169 - acc: 0.9412 - val_loss: 0.3749 - val_acc: 0.8735\n",
            "Epoch 8/20\n",
            "25000/25000 [==============================] - 4s 161us/step - loss: 0.2102 - acc: 0.9443 - val_loss: 0.3725 - val_acc: 0.8754\n",
            "Epoch 9/20\n",
            "25000/25000 [==============================] - 4s 160us/step - loss: 0.2042 - acc: 0.9458 - val_loss: 0.3764 - val_acc: 0.8754\n",
            "Epoch 10/20\n",
            "25000/25000 [==============================] - 4s 159us/step - loss: 0.2004 - acc: 0.9496 - val_loss: 0.3821 - val_acc: 0.8744\n",
            "Epoch 11/20\n",
            "25000/25000 [==============================] - 4s 160us/step - loss: 0.1949 - acc: 0.9494 - val_loss: 0.3836 - val_acc: 0.8755\n",
            "Epoch 12/20\n",
            "25000/25000 [==============================] - 4s 159us/step - loss: 0.1921 - acc: 0.9522 - val_loss: 0.3951 - val_acc: 0.8726\n",
            "Epoch 13/20\n",
            "25000/25000 [==============================] - 4s 160us/step - loss: 0.1880 - acc: 0.9538 - val_loss: 0.3971 - val_acc: 0.8724\n",
            "Epoch 14/20\n",
            "25000/25000 [==============================] - 4s 159us/step - loss: 0.1817 - acc: 0.9566 - val_loss: 0.4184 - val_acc: 0.8678\n",
            "Epoch 15/20\n",
            "25000/25000 [==============================] - 4s 161us/step - loss: 0.1818 - acc: 0.9558 - val_loss: 0.4530 - val_acc: 0.8573\n",
            "Epoch 16/20\n",
            "25000/25000 [==============================] - 4s 161us/step - loss: 0.1766 - acc: 0.9592 - val_loss: 0.4363 - val_acc: 0.8667\n",
            "Epoch 17/20\n",
            "25000/25000 [==============================] - 4s 159us/step - loss: 0.1722 - acc: 0.9605 - val_loss: 0.4234 - val_acc: 0.8691\n",
            "Epoch 18/20\n",
            "25000/25000 [==============================] - 4s 158us/step - loss: 0.1706 - acc: 0.9611 - val_loss: 0.4388 - val_acc: 0.8677\n",
            "Epoch 19/20\n",
            "25000/25000 [==============================] - 4s 161us/step - loss: 0.1666 - acc: 0.9630 - val_loss: 0.4378 - val_acc: 0.8664\n",
            "Epoch 20/20\n",
            "25000/25000 [==============================] - 4s 159us/step - loss: 0.1632 - acc: 0.9651 - val_loss: 0.4303 - val_acc: 0.8685\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxQLxT8fS0qX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "d5335e7e-d5bb-4b04-83c6-63668b4171de"
      },
      "source": [
        "l2_model_val_loss = l2_model_hist.history['val_loss']\n",
        "\n",
        "plt.plot(epochs, original_val_loss, 'ro', label='Original model')\n",
        "plt.plot(epochs, l2_model_val_loss, 'bo', label='L2-regularized model')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFaCAYAAAA3jtULAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNSFqCE8S0qb"
      },
      "source": [
        "As you can see, the model with L2 regularization (dots) has become much more resistant to overfitting than the reference model (crosses), \n",
        "even though both models have the same number of parameters.\n",
        "\n",
        "__Try yourself__\n",
        "As alternatives to L2 regularization, you could use one of the following Keras weight regularizers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3brVod-3S0qc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "286374ec-8d11-4644-c26c-06f1c6ed7cd3"
      },
      "source": [
        "from keras import regularizers\n",
        "\n",
        "# L1 regularization\n",
        "regularizers.l1(0.001)\n",
        "\n",
        "# L1 and L2 regularization at the same time\n",
        "regularizers.l1_l2(l1=0.001, l2=0.001)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.regularizers.L1L2 at 0x7fb653699898>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tl4K5Sw8S0qh"
      },
      "source": [
        "## Adding dropout\n",
        "\n",
        "\n",
        "Dropout is one of the most effective and most commonly used regularization techniques for neural networks, developed by Hinton and his \n",
        "students at the University of Toronto. Dropout, applied to a layer, consists of randomly \"dropping out\" (i.e. setting to zero) a number of \n",
        "output features of the layer during training. Let's say a given layer would normally have returned a vector `[0.2, 0.5, 1.3, 0.8, 1.1]` for a given input sample during training; after applying dropout, this vector will have a few zero entries distributed at random, e.g. `[0, 0.5, \n",
        "1.3, 0, 1.1]`. The \"dropout rate\" is the fraction of the features that are being zeroed-out; it is usually set between 0.2 and 0.5. \n",
        "\n",
        "_At test time, no units are dropped out, and instead the layer's output values are scaled down by a factor equal to the dropout rate, so as to balance for the fact that more units are active than at training time._\n",
        "\n",
        "Consider a Numpy matrix containing the output of a layer, `layer_output`, of shape `(batch_size, features)`. At training time, we would be zero-ing out at random a fraction of the values in the matrix:\n",
        "\n",
        "__Remember__ randint is sampling a discreter uniform distribution. For low=0, high=2, then it generates 0 or 1, each with uniform prob = 50%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30cPEbOlfe_i"
      },
      "source": [
        "```\n",
        "# At training time: we drop out 50% of the units in the output\n",
        "layer_output *= np.randint(0, high=2, size=layer_output.shape)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gshD6VepS0ql"
      },
      "source": [
        "\n",
        "At test time, we would be scaling the output down by the dropout rate. Here we scale by 0.5 (because we were previous dropping half the \n",
        "units):\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySaTpFllfh_O"
      },
      "source": [
        "```\n",
        "# At test time:\n",
        "layer_output *= 0.5\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUHc_WNfS0qs"
      },
      "source": [
        "\n",
        "Note that this process can be implemented by doing both operations at training time and leaving the output unchanged at test time, which is \n",
        "often the way it is implemented in practice:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9L1LZ10fk9I"
      },
      "source": [
        "```\n",
        "# At training time:\n",
        "layer_output *= np.randint(0, high=2, size=layer_output.shape)\n",
        "# Note that we are scaling *up* rather scaling *down* in this case\n",
        "layer_output /= 0.5\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLaVwjC8S0q1"
      },
      "source": [
        "__In keras it's as simple as__:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_Hka7Nqfrdp"
      },
      "source": [
        "```\n",
        "model.add(layers.Dropout(0.5))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgdK2VU0S0q8"
      },
      "source": [
        "__Now let's apply to movie review__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wX8XDXArS0q-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "0ae5cc9a-721d-4ed9-fa95-5948869b2937"
      },
      "source": [
        "dpt_model = models.Sequential()\n",
        "dpt_model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
        "dpt_model.add(layers.Dropout(0.5))\n",
        "dpt_model.add(layers.Dense(16, activation='relu'))\n",
        "dpt_model.add(layers.Dropout(0.5))\n",
        "dpt_model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "dpt_model.compile(optimizer='rmsprop',\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['acc'])\n",
        "dpt_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_28 (Dense)             (None, 16)                160016    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_29 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_30 (Dense)             (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 160,305\n",
            "Trainable params: 160,305\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5-pXzl9S0rB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        },
        "outputId": "2d18bebd-eec9-4d05-fdbb-473959a78550"
      },
      "source": [
        "dpt_model_hist = dpt_model.fit(x_train, y_train,\n",
        "                               epochs=20,\n",
        "                               batch_size=512,\n",
        "                               validation_data=(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/20\n",
            "25000/25000 [==============================] - 5s 193us/step - loss: 0.5911 - acc: 0.6823 - val_loss: 0.4324 - val_acc: 0.8608\n",
            "Epoch 2/20\n",
            "25000/25000 [==============================] - 4s 159us/step - loss: 0.4368 - acc: 0.8169 - val_loss: 0.3500 - val_acc: 0.8676\n",
            "Epoch 3/20\n",
            "25000/25000 [==============================] - 4s 159us/step - loss: 0.3487 - acc: 0.8714 - val_loss: 0.2911 - val_acc: 0.8871\n",
            "Epoch 4/20\n",
            "25000/25000 [==============================] - 4s 159us/step - loss: 0.2901 - acc: 0.8974 - val_loss: 0.2764 - val_acc: 0.8882\n",
            "Epoch 5/20\n",
            "25000/25000 [==============================] - 4s 167us/step - loss: 0.2560 - acc: 0.9123 - val_loss: 0.2799 - val_acc: 0.8884\n",
            "Epoch 6/20\n",
            "25000/25000 [==============================] - 4s 167us/step - loss: 0.2259 - acc: 0.9248 - val_loss: 0.2886 - val_acc: 0.8867\n",
            "Epoch 7/20\n",
            "25000/25000 [==============================] - 4s 163us/step - loss: 0.2004 - acc: 0.9344 - val_loss: 0.3200 - val_acc: 0.8853\n",
            "Epoch 8/20\n",
            "25000/25000 [==============================] - 4s 161us/step - loss: 0.1824 - acc: 0.9410 - val_loss: 0.3283 - val_acc: 0.8845\n",
            "Epoch 9/20\n",
            "25000/25000 [==============================] - 4s 161us/step - loss: 0.1765 - acc: 0.9438 - val_loss: 0.3556 - val_acc: 0.8832\n",
            "Epoch 10/20\n",
            "25000/25000 [==============================] - 4s 160us/step - loss: 0.1577 - acc: 0.9474 - val_loss: 0.3696 - val_acc: 0.8830\n",
            "Epoch 11/20\n",
            "25000/25000 [==============================] - 4s 158us/step - loss: 0.1495 - acc: 0.9512 - val_loss: 0.3867 - val_acc: 0.8802\n",
            "Epoch 12/20\n",
            "25000/25000 [==============================] - 4s 162us/step - loss: 0.1414 - acc: 0.9550 - val_loss: 0.4382 - val_acc: 0.8731\n",
            "Epoch 13/20\n",
            "25000/25000 [==============================] - 4s 160us/step - loss: 0.1279 - acc: 0.9590 - val_loss: 0.4387 - val_acc: 0.8780\n",
            "Epoch 14/20\n",
            "25000/25000 [==============================] - 4s 160us/step - loss: 0.1238 - acc: 0.9589 - val_loss: 0.4535 - val_acc: 0.8759\n",
            "Epoch 15/20\n",
            "25000/25000 [==============================] - 4s 161us/step - loss: 0.1173 - acc: 0.9616 - val_loss: 0.4855 - val_acc: 0.8752\n",
            "Epoch 16/20\n",
            "25000/25000 [==============================] - 4s 161us/step - loss: 0.1178 - acc: 0.9616 - val_loss: 0.5332 - val_acc: 0.8746\n",
            "Epoch 17/20\n",
            "25000/25000 [==============================] - 4s 161us/step - loss: 0.1130 - acc: 0.9634 - val_loss: 0.5199 - val_acc: 0.8756\n",
            "Epoch 18/20\n",
            "25000/25000 [==============================] - 4s 161us/step - loss: 0.1139 - acc: 0.9650 - val_loss: 0.5460 - val_acc: 0.8746\n",
            "Epoch 19/20\n",
            "25000/25000 [==============================] - 4s 161us/step - loss: 0.1004 - acc: 0.9670 - val_loss: 0.5715 - val_acc: 0.8732\n",
            "Epoch 20/20\n",
            "25000/25000 [==============================] - 4s 161us/step - loss: 0.1037 - acc: 0.9680 - val_loss: 0.5660 - val_acc: 0.8707\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQUNR5_0S0rE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "209316cd-67ee-4b3c-a343-dfc12cde0e10"
      },
      "source": [
        "dpt_model_val_loss = dpt_model_hist.history['val_loss']\n",
        "\n",
        "plt.plot(epochs, original_val_loss, 'ro', label='Original model')\n",
        "plt.plot(epochs, dpt_model_val_loss, 'bo', label='Dropout-regularized model')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFaCAYAAAA3jtULAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rJLa_WWS0rF"
      },
      "source": [
        "# Putting it all together: The universal ML workflow\n",
        "___Seven steps:___\n",
        "\n",
        "## 1. Defining the problem and assembling a dataset\n",
        "\n",
        "A. Assemble raw data\n",
        "\n",
        "B. Define the type of problem: classification, multi-label, multi-class, regression,....etc\n",
        "\n",
        "C. Label data \n",
        "\n",
        "\n",
        "Be aware of the hypotheses you make at this stage:\n",
        "- You hypothesize that your outputs can be predicted given your inputs.\n",
        "- You hypothesize that your available data is sufficiently informative to learn the relationship between inputs and outputs.\n",
        "\n",
        "Until you have a working model, these are merely hypotheses, waiting to be validated\n",
        "or invalidated.\n",
        "\n",
        "Not all problems can be solved; just because you’ve assembled examples\n",
        "of inputs X and targets Y doesn’t mean X contains enough information to predict\n",
        "Y. For instance, if you’re trying to predict the movements of a stock on the stock market\n",
        "given its recent price history, you’re unlikely to succeed, because price history\n",
        "doesn’t contain much predictive information.\n",
        "\n",
        "One class of unsolvable problems you should be aware of is ___nonstationary problems___.\n",
        "Suppose you’re trying to build a recommendation engine for clothing, you’re training\n",
        "it on one month of data (August), and you want to start generating recommendations\n",
        "in the winter. One big issue is that the kinds of clothes people buy change from season\n",
        "to season: clothes buying is a nonstationary phenomenon over the scale of a few\n",
        "months. What you’re trying to model changes over time. In this case, the right move is\n",
        "to constantly retrain your model on data from the recent past, or gather data at a\n",
        "timescale where the problem is stationary. For a cyclical problem like clothes buying, a\n",
        "few years’ worth of data will suffice to capture seasonal variation—but remember to\n",
        "make the time of the year an input of your model!\n",
        "\n",
        "Keep in mind that machine learning can only be used to memorize patterns that\n",
        "are present in your training data. __You can only recognize what you’ve seen before.\n",
        "Using machine learning trained on past data to predict the future is making the\n",
        "assumption that the future will behave like the past. That often isn’t the case.__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCc6GwPMS0rG"
      },
      "source": [
        "## 2. Choosing a measure of success\n",
        "__To control something, you need to be able to observe it.__\n",
        "\n",
        "To achieve success, you must define what you mean by success—accuracy? Precision and recall? Customer-retention\n",
        "rate? \n",
        "\n",
        "__A Metric must align with loss__\n",
        "Your metric for success will guide the choice of a loss function: what your model\n",
        "will optimize. It should directly align with your higher-level goals, such as the success\n",
        "of your business.\n",
        "\n",
        "__Common metrics__\n",
        "For _balanced-classification problems_, where every class is equally likely, accuracy and\n",
        "area under the receiver operating characteristic curve (ROC AUC) are common metrics. \n",
        "\n",
        "For _class-imbalanced problems_, you can use precision and recall. For ranking problems or\n",
        "multilabel classification, you can use mean average precision. \n",
        "\n",
        "And it isn’t uncommon to have to define your own custom metric by which to measure success. To get a sense\n",
        "of the diversity of machine-learning success metrics and how they relate to different\n",
        "problem domains, it’s helpful to browse the data science competitions on Kaggle\n",
        "(https://kaggle.com); they showcase a wide range of problems and evaluation metrics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IO4TowTS0rH"
      },
      "source": [
        "# 3. Deciding on an evaluation protocol\n",
        "\n",
        "Once you know what you’re aiming for, you must establish how you’ll measure your\n",
        "current progress. We’ve previously reviewed three common evaluation protocols:\n",
        "\n",
        "- Maintaining a hold-out validation set—The way to go when you have plenty of\n",
        "data\n",
        "\n",
        "- Doing K-fold cross-validation—The right choice when you have too few samples\n",
        "for hold-out validation to be reliable\n",
        "\n",
        "- Doing iterated K-fold validation—For performing highly accurate model evaluation\n",
        "when little data is available. But it's very expensive in terms of time.\n",
        "\n",
        "Just pick one of these. In most cases, the first will work well enough."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZWBEtx6S0rH"
      },
      "source": [
        "# 4. Preparing your data\n",
        "\n",
        "- As you saw previously, your data should be formatted as tensors.\n",
        "\n",
        "- The values taken by these tensors should usually be scaled to small values: for example, in the [-1, 1] range or [0, 1] range.\n",
        "\n",
        "- If different features take values in different ranges (heterogeneous data), then the data should be normalized.\n",
        "\n",
        "- You may want to do some feature engineering, especially for small-data problems.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Epi-CaGLS0rI"
      },
      "source": [
        "# 5. Developing a model that does better than a baseline\n",
        "Your goal at this stage is to achieve ___statistical power___ (1/num_classes) : that is, to develop a small model\n",
        "that is capable of beating a dumb baseline. In the MNIST digit-classification example,\n",
        "anything that achieves an accuracy greater than 0.1 can be said to have statistical\n",
        "power; in the IMDB example, it’s anything with an accuracy greater than 0.5.\n",
        "\n",
        "Note that it’s not always possible to achieve statistical power. If you can’t beat a random\n",
        "baseline after trying multiple reasonable architectures, it may be that the answer\n",
        "to the question you’re asking isn’t present in the input data. Remember that you make\n",
        "two hypotheses:\n",
        "- You hypothesize that your outputs can be predicted given your inputs.\n",
        "- You hypothesize that the available data is sufficiently informative to learn the\n",
        "relationship between inputs and outputs.\n",
        "\n",
        "\n",
        "It may well be that these hypotheses are false, in which case you must go back to the\n",
        "drawing board.\n",
        "\n",
        "\n",
        "Assuming that things go well, you need to make three key choices to build your\n",
        "first working model:\n",
        "- _Last-layer activation_—This establishes useful constraints on the network’s output.\n",
        "For instance, the IMDB classification example used sigmoid in the last\n",
        "layer; the regression example didn’t use any last-layer activation; and so on.\n",
        "\n",
        "\n",
        "- _Loss function_—This should match the type of problem you’re trying to solve. For\n",
        "instance, the IMDB example used binary_crossentropy, the regression example\n",
        "used mse, and so on.\n",
        "\n",
        "![04_7_last_layer_activation_and_loss.png](https://github.com/ahmadelsallab/practical_dl/blob/master/Keras/notebooks/imgs/04_7_last_layer_activation_and_loss.png?raw=true)\n",
        "\n",
        "__Metrics are not always possible to optimize for__\n",
        "\n",
        "Regarding the choice of a loss function, note that it isn’t always possible to directly\n",
        "optimize for the metric that measures success on a problem. Sometimes there is no\n",
        "easy way to turn a metric into a loss function; loss functions, after all, need to be:\n",
        "\n",
        "1- computablegiven only a mini-batch of data (ideally, a loss function should be computable\n",
        "for as little as a single data point) and \n",
        "\n",
        "2- must be differentiable (otherwise, you can’t usebackpropagation to train your network). \n",
        "\n",
        "For instance, the widely used classification metric ROC AUC can’t be directly optimized. Hence, in classification tasks, it’s common to optimize for a proxy metric of ROC AUC, such as crossentropy. In general, you can hope that the lower the crossentropy gets, the higher the ROC AUC will be.\n",
        "\n",
        "\n",
        "- _Optimization configuration_—What optimizer will you use? What will its learning\n",
        "rate be? In most cases, it’s safe to go with rmsprop or adam and their default learning rate.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzGqRG2sS0rI"
      },
      "source": [
        "# 6. Scaling up: developing a model that overfits\n",
        "Once you’ve obtained a model that has statistical power, the question becomes, is your model sufficiently powerful?\n",
        "Does it have enough layers and parameters to properly model the problem at hand? \n",
        "\n",
        "For instance, a network with a single hidden layer with\n",
        "two units would have statistical power on MNIST but wouldn’t be sufficient to solve the\n",
        "problem well. \n",
        "\n",
        "Remember that the universal tension in machine learning is between\n",
        "optimization and generalization; the ideal model is one that stands right at the border\n",
        "between underfitting and overfitting; between undercapacity and overcapacity. \n",
        "\n",
        "___To figure out where this border lies, first you must cross it.___\n",
        "\n",
        "_To figure out how big a model you’ll need, you must develop a model that overfits._\n",
        "\n",
        "This is fairly easy:\n",
        "1. Add layers.\n",
        "\n",
        "2. Make the layers bigger.\n",
        "\n",
        "3. Train for more epochs.\n",
        "\n",
        "Always monitor the training loss and validation loss, as well as the training and validation\n",
        "values for any metrics you care about. When you see that the model’s performance\n",
        "on the validation data begins to degrade, you’ve achieved overfitting.\n",
        "\n",
        "__The next stage is to start regularizing and tuning the model, to get as close as possible\n",
        "to the ideal model that neither underfits nor overfits.__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9W4DdCRRS0rI"
      },
      "source": [
        "# 7. Regularizing your model and tuning your hyperparameters\n",
        "This step will take the most time: you’ll repeatedly modify your model, train it, evaluate\n",
        "on your validation data (not the test data, at this point), modify it again, and\n",
        "repeat, until the model is as good as it can get. These are some things you should try:\n",
        "\n",
        "- Try to increase data (not always possible).\n",
        "- Try data augmentation (advanced).\n",
        "- Try transfer learning (advanced). \n",
        "- Add dropout.\n",
        "- Add L1 and/or L2 regularization.\n",
        "- Try different hyperparameters (such as the number of units per layer or the learning rate of the optimizer) to find the optimal configuration.\n",
        "- Optionally, iterate on feature engineering: add new features, or remove features that don’t seem to be informative.\n",
        "- Try different architectures: add or remove layers.\n",
        "\n",
        "Once you’ve developed a satisfactory model configuration, you can train your final\n",
        "production model on all the available data (training and validation) and evaluate it\n",
        "one last time on the test set. \n",
        "\n",
        "_If it turns out that performance on the test set is significantly\n",
        "worse than the performance measured on the validation data, this may mean\n",
        "either that your validation procedure wasn’t reliable after all, or that you began overfitting\n",
        "to the validation data while tuning the parameters of the model._\n",
        "\n",
        "___In this case, you may want to switch to a more reliable evaluation protocol (such as iterated K-fold\n",
        "validation).___\n"
      ]
    }
  ]
}